diff --git a/sdk/android/api/org/webrtc/EncodedImage.java b/sdk/android/api/org/webrtc/EncodedImage.java
index a6eef67..6896205 100644
--- a/sdk/android/api/org/webrtc/EncodedImage.java
+++ b/sdk/android/api/org/webrtc/EncodedImage.java
@@ -56,6 +56,31 @@ public class EncodedImage implements RefCounted {
   public final int rotation;
   public final @Nullable Integer qp;
 
+  //START:RTC_ENABLE_BFRAME
+  public long timestampRtp;
+  public long decodingTimestamp;
+  
+  @CalledByNative
+  public void setTimestampRtp(long timestampRtp) {
+    this.timestampRtp = timestampRtp;
+  }
+
+  @CalledByNative
+  public long getTimestampRtp() {
+    return this.timestampRtp;
+  }
+
+  public void setDecodingTimestamp(long timestamp) {
+    this.decodingTimestamp = timestamp;
+  }
+
+  @CalledByNative
+  public long getDecodingTimestamp() {
+    return this.decodingTimestamp;
+  }
+  //END:RTC_ENABLE_BFRAME
+
+
   // TODO(bugs.webrtc.org/9378): Use retain and release from jni code.
   @Override
   public void retain() {
@@ -130,7 +155,6 @@ public class EncodedImage implements RefCounted {
     private EncodedImage.FrameType frameType;
     private int rotation;
     private @Nullable Integer qp;
-
     private Builder() {}
 
     public Builder setBuffer(ByteBuffer buffer, @Nullable Runnable releaseCallback) {
@@ -175,9 +199,34 @@ public class EncodedImage implements RefCounted {
       return this;
     }
 
+    //START:RTC_ENABLE_BFRAME
+    private long timestampRtp;
+    public Builder setTimestampRtp(long timestampRtp) {
+      this.timestampRtp = timestampRtp;
+      return this;
+    }
+    public long getTimestampRtp() {
+      return this.timestampRtp;
+    }
+    private long decodingTimestamp;
+
+    public Builder setDecodingTimestamp(long decodingTimestamp) {
+      this.decodingTimestamp = decodingTimestamp;
+      return this;
+    }
+    //END:RTC_ENABLE_BFRAME
+
     public EncodedImage createEncodedImage() {
-      return new EncodedImage(buffer, releaseCallback, encodedWidth, encodedHeight, captureTimeNs,
-          frameType, rotation, qp);
+
+      EncodedImage encodedImage =  new EncodedImage(buffer, releaseCallback, encodedWidth, 
+        encodedHeight, captureTimeNs,frameType, rotation, qp);
+
+      //START:RTC_ENABLE_BFRAME
+      encodedImage.setTimestampRtp(timestampRtp);
+      encodedImage.setDecodingTimestamp(decodingTimestamp);
+      //END:RTC_ENABLE_BFRAME
+
+      return encodedImage;
     }
   }
 }
diff --git a/sdk/android/api/org/webrtc/HardwareVideoEncoderFactory.java b/sdk/android/api/org/webrtc/HardwareVideoEncoderFactory.java
index 8cf6714..e57ebf0 100644
--- a/sdk/android/api/org/webrtc/HardwareVideoEncoderFactory.java
+++ b/sdk/android/api/org/webrtc/HardwareVideoEncoderFactory.java
@@ -13,6 +13,9 @@ package org.webrtc;
 import static org.webrtc.MediaCodecUtils.EXYNOS_PREFIX;
 import static org.webrtc.MediaCodecUtils.INTEL_PREFIX;
 import static org.webrtc.MediaCodecUtils.QCOM_PREFIX;
+import static org.webrtc.MediaCodecUtils.QTI_PREFIX;
+import static org.webrtc.MediaCodecUtils.C2_EXYNOS_PREFIX;
+
 
 import android.media.MediaCodecInfo;
 import android.media.MediaCodecList;
@@ -46,6 +49,9 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
   @Nullable private final EglBase14.Context sharedContext;
   private final boolean enableIntelVp8Encoder;
   private final boolean enableH264HighProfile;
+  //START:RTC_ENABLE_BFRAME
+  private final int maxBframes;
+  //END:RTC_ENABLE_BFRAME
   @Nullable private final Predicate<MediaCodecInfo> codecAllowedPredicate;
 
   /**
@@ -55,10 +61,13 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
    *                      this disables texture support.
    * @param enableIntelVp8Encoder true if Intel's VP8 encoder enabled.
    * @param enableH264HighProfile true if H264 High Profile enabled.
+   * @param maxBframes 0 is disabled, 1 or more activates B frames and specifies the number.
+   * @param codecAllowedPredicate optional predicate to filter codecs. All codecs are allowed
+   *                              when predicate is not provided.
    */
   public HardwareVideoEncoderFactory(
       EglBase.Context sharedContext, boolean enableIntelVp8Encoder, boolean enableH264HighProfile) {
-    this(sharedContext, enableIntelVp8Encoder, enableH264HighProfile,
+    this(sharedContext, enableIntelVp8Encoder, enableH264HighProfile, 0, 
         /* codecAllowedPredicate= */ null);
   }
 
@@ -74,6 +83,16 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
    */
   public HardwareVideoEncoderFactory(EglBase.Context sharedContext, boolean enableIntelVp8Encoder,
       boolean enableH264HighProfile, @Nullable Predicate<MediaCodecInfo> codecAllowedPredicate) {
+    this(sharedContext, enableIntelVp8Encoder, enableH264HighProfile, /*maxBframes*/0, codecAllowedPredicate);
+  }
+
+  public HardwareVideoEncoderFactory(
+      EglBase.Context sharedContext, boolean enableIntelVp8Encoder, boolean enableH264HighProfile, int maxBframes) {
+    this(sharedContext, enableIntelVp8Encoder, enableH264HighProfile, maxBframes, /* codecAllowedPredicate= */ null);
+  }
+
+  public HardwareVideoEncoderFactory(EglBase.Context sharedContext, boolean enableIntelVp8Encoder,
+  boolean enableH264HighProfile, int maxBframes, @Nullable Predicate<MediaCodecInfo> codecAllowedPredicate) {
     // Texture mode requires EglBase14.
     if (sharedContext instanceof EglBase14.Context) {
       this.sharedContext = (EglBase14.Context) sharedContext;
@@ -84,8 +103,10 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
     this.enableIntelVp8Encoder = enableIntelVp8Encoder;
     this.enableH264HighProfile = enableH264HighProfile;
     this.codecAllowedPredicate = codecAllowedPredicate;
+    this.maxBframes = maxBframes;
   }
 
+
   @Deprecated
   public HardwareVideoEncoderFactory(boolean enableIntelVp8Encoder, boolean enableH264HighProfile) {
     this(null, enableIntelVp8Encoder, enableH264HighProfile);
@@ -113,7 +134,9 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
           input.params, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ true));
       boolean isBaselineProfile = H264Utils.isSameH264Profile(
           input.params, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ false));
-
+      Logging.d(TAG, "isHighProfile:" + isHighProfile + ", isBaselineProfile:" + isBaselineProfile + 
+        ", isH264HighProfileSupported:" +  isH264HighProfileSupported(info) + ", maxBFrames:" + this.maxBframes);
+ 
       if (!isHighProfile && !isBaselineProfile) {
         return null;
       }
@@ -124,7 +147,7 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
 
     return new HardwareVideoEncoder(new MediaCodecWrapperFactoryImpl(), codecName, type,
         surfaceColorFormat, yuvColorFormat, input.params, PERIODIC_KEY_FRAME_INTERVAL_S,
-        getForcedKeyFrameIntervalMs(type, codecName), createBitrateAdjuster(type, codecName),
+        getForcedKeyFrameIntervalMs(type, codecName), this.maxBframes, createBitrateAdjuster(type, codecName),
         sharedContext);
   }
 
@@ -142,8 +165,27 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
         // TODO(sakal): Always add H264 HP once WebRTC correctly removes codecs that are not
         // supported by the decoder.
         if (type == VideoCodecMimeType.H264 && isH264HighProfileSupported(codec)) {
-          supportedCodecInfos.add(new VideoCodecInfo(
-              name, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ true)));
+          VideoCodecInfo info = new VideoCodecInfo(
+            name, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ true));
+          //START:RTC_ENABLE_BFRAME
+          if(this.maxBframes > 0)
+          {
+            info.setBframeEnabled(true);
+          }
+          //END:RTC_ENABLE_BFRAME
+          supportedCodecInfos.add(info);
+        }
+        else 
+        if (type == VideoCodecMimeType.H265) {
+          VideoCodecInfo info = new VideoCodecInfo(
+            name, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ true));
+          //START:RTC_ENABLE_BFRAME
+          if(this.maxBframes > 0)
+          {
+            info.setBframeEnabled(true);
+          }
+          //END:RTC_ENABLE_BFRAME
+          supportedCodecInfos.add(info);
         }
 
         supportedCodecInfos.add(new VideoCodecInfo(
@@ -278,6 +320,9 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
 
   private boolean isH264HighProfileSupported(MediaCodecInfo info) {
     return enableH264HighProfile && Build.VERSION.SDK_INT > Build.VERSION_CODES.M
-        && info.getName().startsWith(EXYNOS_PREFIX);
+        && (info.getName().startsWith(EXYNOS_PREFIX)
+            || info.getName().startsWith(QTI_PREFIX)
+            || info.getName().startsWith(QCOM_PREFIX)
+            || info.getName().startsWith(C2_EXYNOS_PREFIX));
   }
 }
diff --git a/sdk/android/api/org/webrtc/VideoCodecInfo.java b/sdk/android/api/org/webrtc/VideoCodecInfo.java
index 86d67d6..8307d71 100644
--- a/sdk/android/api/org/webrtc/VideoCodecInfo.java
+++ b/sdk/android/api/org/webrtc/VideoCodecInfo.java
@@ -35,6 +35,10 @@ public class VideoCodecInfo {
   public final String name;
   public final Map<String, String> params;
   public int[] scalabilityModes;
+
+  //START:RTC_ENABLE_BFRAME
+  public boolean bframeEnabled;
+  //END:RTC_ENABLE_BFRAME
   @Deprecated public final int payload;
 
   @CalledByNative
@@ -97,5 +101,15 @@ public class VideoCodecInfo {
     scalabilityModes = values;
   }
 
+  //START:RTC_ENABLE_BFRAME
+  @CalledByNative
+  boolean getBframeEnabled() {
+    return bframeEnabled;
+  }
 
+  @CalledByNative
+  void setBframeEnabled(boolean value) {
+    bframeEnabled = value;
+  }  
+  //END:RTC_ENABLE_BFRAME
 }
diff --git a/sdk/android/api/org/webrtc/VideoEncoder.java b/sdk/android/api/org/webrtc/VideoEncoder.java
index be62686..6c7c795 100644
--- a/sdk/android/api/org/webrtc/VideoEncoder.java
+++ b/sdk/android/api/org/webrtc/VideoEncoder.java
@@ -29,18 +29,20 @@ public interface VideoEncoder {
     public final int numberOfSimulcastStreams;
     public final boolean automaticResizeOn;
     public final Capabilities capabilities;
-
+    //START:RTC_ENABLE_BFRAME
+    public final boolean bframesEnabled;
+    //END:RTC_ENABLE_BFRAME
     // TODO(bugs.webrtc.org/10720): Remove.
     @Deprecated
     public Settings(int numberOfCores, int width, int height, int startBitrate, int maxFramerate,
         int numberOfSimulcastStreams, boolean automaticResizeOn) {
       this(numberOfCores, width, height, startBitrate, maxFramerate, numberOfSimulcastStreams,
-          automaticResizeOn, new VideoEncoder.Capabilities(false /* lossNotification */));
+          automaticResizeOn, new VideoEncoder.Capabilities(false /* lossNotification */), false);
     }
 
     @CalledByNative("Settings")
     public Settings(int numberOfCores, int width, int height, int startBitrate, int maxFramerate,
-        int numberOfSimulcastStreams, boolean automaticResizeOn, Capabilities capabilities) {
+        int numberOfSimulcastStreams, boolean automaticResizeOn, Capabilities capabilities, boolean bframesEnabled) {
       this.numberOfCores = numberOfCores;
       this.width = width;
       this.height = height;
@@ -49,6 +51,9 @@ public interface VideoEncoder {
       this.numberOfSimulcastStreams = numberOfSimulcastStreams;
       this.automaticResizeOn = automaticResizeOn;
       this.capabilities = capabilities;
+      //START:RTC_ENABLE_BFRAME
+      this.bframesEnabled = bframesEnabled;
+      //END:RTC_ENABLE_BFRAME
     }
   }
 
diff --git a/sdk/android/api/org/webrtc/VideoFrame.java b/sdk/android/api/org/webrtc/VideoFrame.java
index 443a031..fd8fc4c 100644
--- a/sdk/android/api/org/webrtc/VideoFrame.java
+++ b/sdk/android/api/org/webrtc/VideoFrame.java
@@ -231,4 +231,20 @@ public class VideoFrame implements RefCounted {
   public void release() {
     buffer.release();
   }
+
+  //START:RTC_ENABLE_BFRAME
+  private long timestampRtp;
+
+  @CalledByNative
+  public void setTimestampRtp(long timestampRtp)
+  {
+    this.timestampRtp = timestampRtp;
+  }
+
+  @CalledByNative
+  public long getTimestampRtp()
+  {
+    return this.timestampRtp;
+  }
+  //END:RTC_ENABLE_BFRAME
 }
diff --git a/sdk/android/instrumentationtests/src/org/webrtc/AndroidVideoDecoderInstrumentationTest.java b/sdk/android/instrumentationtests/src/org/webrtc/AndroidVideoDecoderInstrumentationTest.java
index 6f44812..61a4d21 100644
--- a/sdk/android/instrumentationtests/src/org/webrtc/AndroidVideoDecoderInstrumentationTest.java
+++ b/sdk/android/instrumentationtests/src/org/webrtc/AndroidVideoDecoderInstrumentationTest.java
@@ -65,7 +65,7 @@ public final class AndroidVideoDecoderInstrumentationTest {
   private static final VideoEncoder.Settings ENCODER_SETTINGS =
       new VideoEncoder.Settings(1 /* core */, TEST_FRAME_WIDTH, TEST_FRAME_HEIGHT, 300 /* kbps */,
           30 /* fps */, 1 /* numberOfSimulcastStreams */, true /* automaticResizeOn */,
-          /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */));
+          /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */), /* bFramesEnabled */false);
 
   private static final int DECODE_TIMEOUT_MS = 1000;
   private static final VideoDecoder.Settings SETTINGS =
diff --git a/sdk/android/instrumentationtests/src/org/webrtc/HardwareVideoEncoderTest.java b/sdk/android/instrumentationtests/src/org/webrtc/HardwareVideoEncoderTest.java
index ab8490e..695fc3f 100644
--- a/sdk/android/instrumentationtests/src/org/webrtc/HardwareVideoEncoderTest.java
+++ b/sdk/android/instrumentationtests/src/org/webrtc/HardwareVideoEncoderTest.java
@@ -59,7 +59,7 @@ public class HardwareVideoEncoderTest {
   private static final VideoEncoder.Settings SETTINGS =
       new VideoEncoder.Settings(1 /* core */, 640 /* width */, 480 /* height */, 300 /* kbps */,
           30 /* fps */, 1 /* numberOfSimulcastStreams */, true /* automaticResizeOn */,
-          /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */));
+          /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */), /* bFramesEnabled */false);
   private static final int ENCODE_TIMEOUT_MS = 1000;
   private static final int NUM_TEST_FRAMES = 10;
   private static final int NUM_ENCODE_TRIES = 100;
diff --git a/sdk/android/src/java/org/webrtc/AndroidVideoDecoder.java b/sdk/android/src/java/org/webrtc/AndroidVideoDecoder.java
index 47cb568..24803e2 100644
--- a/sdk/android/src/java/org/webrtc/AndroidVideoDecoder.java
+++ b/sdk/android/src/java/org/webrtc/AndroidVideoDecoder.java
@@ -267,6 +267,7 @@ class AndroidVideoDecoder implements VideoDecoder, VideoSink {
 
     frameInfos.offer(new FrameInfo(SystemClock.elapsedRealtime(), frame.rotation));
     try {
+
       codec.queueInputBuffer(index, 0 /* offset */, size,
           TimeUnit.NANOSECONDS.toMicros(frame.captureTimeNs), 0 /* flags */);
     } catch (IllegalStateException e) {
@@ -409,6 +410,21 @@ class AndroidVideoDecoder implements VideoDecoder, VideoSink {
     }
 
     synchronized (renderedTextureMetadataLock) {
+      //START:RTC_ENABLE_BFRAME
+      // To prevent frames from being burst-generated when decoding B frames, 
+      // causing frame drops on the surface, wait until onFrame is called.
+      try {
+        long timeoutMillis = TimeUnit.MILLISECONDS.convert(DEQUEUE_OUTPUT_BUFFER_TIMEOUT_US, TimeUnit.MICROSECONDS);
+        long startTime = System.currentTimeMillis();
+        long elapsedTime = 0;
+        while(renderedTextureMetadata != null && elapsedTime < timeoutMillis) {
+          long waitTime = timeoutMillis - elapsedTime;
+          renderedTextureMetadataLock.wait(waitTime);
+          elapsedTime = System.currentTimeMillis() - startTime;
+        }
+      } catch(InterruptedException  e) { }
+      //END:RTC_ENABLE_BFRAME
+
       if (renderedTextureMetadata != null) {
         codec.releaseOutputBuffer(index, false);
         return; // We are still waiting for texture for the previous frame, drop this one.
@@ -433,6 +449,11 @@ class AndroidVideoDecoder implements VideoDecoder, VideoSink {
       timestampNs = renderedTextureMetadata.presentationTimestampUs * 1000;
       decodeTimeMs = renderedTextureMetadata.decodeTimeMs;
       renderedTextureMetadata = null;
+
+      //START:RTC_ENABLE_BFRAME
+      renderedTextureMetadataLock.notifyAll();
+      //END:RTC_ENABLE_BFRAME
+
     }
     // Change timestamp of frame.
     final VideoFrame frameWithModifiedTimeStamp =
diff --git a/sdk/android/src/java/org/webrtc/HardwareVideoEncoder.java b/sdk/android/src/java/org/webrtc/HardwareVideoEncoder.java
index 9f57b20..8a6f1b4 100644
--- a/sdk/android/src/java/org/webrtc/HardwareVideoEncoder.java
+++ b/sdk/android/src/java/org/webrtc/HardwareVideoEncoder.java
@@ -13,6 +13,7 @@ package org.webrtc;
 import static android.media.MediaCodecInfo.CodecProfileLevel.AVCLevel3;
 import static android.media.MediaCodecInfo.CodecProfileLevel.AVCProfileHigh;
 import static android.media.MediaCodecInfo.EncoderCapabilities.BITRATE_MODE_CBR;
+import static android.media.MediaCodecInfo.EncoderCapabilities.BITRATE_MODE_VBR;
 
 import android.media.MediaCodec;
 import android.media.MediaCodecInfo;
@@ -29,7 +30,9 @@ import java.util.Map;
 import java.util.concurrent.BlockingDeque;
 import java.util.concurrent.LinkedBlockingDeque;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.ConcurrentHashMap;
 import org.webrtc.ThreadUtils.ThreadChecker;
+import java.util.Vector;
 
 /**
  * Android hardware video encoder.
@@ -40,7 +43,7 @@ class HardwareVideoEncoder implements VideoEncoder {
   private static final int MAX_VIDEO_FRAMERATE = 30;
 
   // See MAX_ENCODER_Q_SIZE in androidmediaencoder.cc.
-  private static final int MAX_ENCODER_Q_SIZE = 2;
+  private static final int MAX_ENCODER_Q_SIZE = 4;
 
   private static final int MEDIA_CODEC_RELEASE_TIMEOUT_MS = 5000;
   private static final int DEQUEUE_OUTPUT_BUFFER_TIMEOUT_US = 100000;
@@ -115,7 +118,13 @@ class HardwareVideoEncoder implements VideoEncoder {
   // A queue of EncodedImage.Builders that correspond to frames in the codec.  These builders are
   // pre-populated with all the information that can't be sent through MediaCodec.
   private final BlockingDeque<EncodedImage.Builder> outputBuilders = new LinkedBlockingDeque<>();
-
+  //START:RTC_ENABLE_BFRAME
+  // When BFrames are enabled, the order of decoded frames changes. 
+  // It is used for the purpose of finding the timestamp RTP value corresponding to the frame.
+  // <presentationTimestamp, timestampRtp>
+  private final ConcurrentHashMap<Long, Long> reorderTimestamp = new ConcurrentHashMap<>();
+  private static final int MAX_TIMESTAMP_Q_SIZE = 16;
+  //END:RTC_ENABLE_BFRAME
   private final ThreadChecker encodeThreadChecker = new ThreadChecker();
   private final ThreadChecker outputThreadChecker = new ThreadChecker();
   private final BusyCount outputBuffersBusyCount = new BusyCount();
@@ -169,6 +178,11 @@ class HardwareVideoEncoder implements VideoEncoder {
   // True if collection of encoding statistics is enabled.
   private boolean isEncodingStatisticsEnabled;
 
+  //START:RTC_ENABLE_BFRAME
+  private boolean bframesEnabled;
+  private int     maxBframes;
+  //END:RTC_ENABLE_BFRAME
+
   /**
    * Creates a new HardwareVideoEncoder with the given codecName, codecType, colorFormat, key frame
    * intervals, and bitrateAdjuster.
@@ -186,7 +200,7 @@ class HardwareVideoEncoder implements VideoEncoder {
    */
   public HardwareVideoEncoder(MediaCodecWrapperFactory mediaCodecWrapperFactory, String codecName,
       VideoCodecMimeType codecType, Integer surfaceColorFormat, Integer yuvColorFormat,
-      Map<String, String> params, int keyFrameIntervalSec, int forceKeyFrameIntervalMs,
+      Map<String, String> params, int keyFrameIntervalSec, int forceKeyFrameIntervalMs, int maxBframes, 
       BitrateAdjuster bitrateAdjuster, EglBase14.Context sharedContext) {
     this.mediaCodecWrapperFactory = mediaCodecWrapperFactory;
     this.codecName = codecName;
@@ -198,7 +212,7 @@ class HardwareVideoEncoder implements VideoEncoder {
     this.forcedKeyFrameNs = TimeUnit.MILLISECONDS.toNanos(forceKeyFrameIntervalMs);
     this.bitrateAdjuster = bitrateAdjuster;
     this.sharedContext = sharedContext;
-
+    this.maxBframes = maxBframes;
     // Allow construction on a different thread.
     encodeThreadChecker.detachThread();
   }
@@ -219,10 +233,16 @@ class HardwareVideoEncoder implements VideoEncoder {
     }
     adjustedBitrate = bitrateAdjuster.getAdjustedBitrateBps();
 
+    //START:RTC_ENABLE_BFRAME
+    this.bframesEnabled = settings.bframesEnabled;
+    //END:RTC_ENABLE_BFRAME
+
     Logging.d(TAG,
         "initEncode name: " + codecName + " type: " + codecType + " width: " + width
             + " height: " + height + " framerate_fps: " + settings.maxFramerate
-            + " bitrate_kbps: " + settings.startBitrate + " surface mode: " + useSurfaceMode);
+            + " bitrate_kbps: " + settings.startBitrate + " surface mode: " + useSurfaceMode
+            + " bframes_enabled: " + settings.bframesEnabled + ", maxBframes: " + this.maxBframes);
+            
     return initEncodeInternal();
   }
 
@@ -256,16 +276,38 @@ class HardwareVideoEncoder implements VideoEncoder {
           profileLevelId = VideoCodecInfo.H264_CONSTRAINED_BASELINE_3_1;
         }
         switch (profileLevelId) {
+          // TODO: In case of High profile, the H264_FMTP_PROFILE_LEVEL_ID value is defined differently 
+          // from the iOS SDK. The value must be matched.
+          case "640c29": // Compatible with Profile High on iOS
           case VideoCodecInfo.H264_CONSTRAINED_HIGH_3_1:
             format.setInteger("profile", AVCProfileHigh);
             format.setInteger("level", AVCLevel3);
+            // START:RTC_ENABLE_BFRAME
+            if (this.bframesEnabled == true) {
+              format.setInteger(MediaFormat.KEY_MAX_B_FRAMES, this.maxBframes);             
+              Logging.w(TAG, String.format("PROFILE_ID: %s, KEY_LEVEL:%d, KEY_MAX_B_FRAMES: %d",
+                 VideoCodecInfo.H264_CONSTRAINED_HIGH_3_1, AVCLevel3, this.maxBframes));              
+            }
+            // END:RTC_ENABLE_BFRAME
             break;
           case VideoCodecInfo.H264_CONSTRAINED_BASELINE_3_1:
+            // START:RTC_ENABLE_BFRAME
+              Logging.w(TAG, String.format("PROFILE_ID: %s, KEY_MAX_B_FRAMES: %d", 
+                VideoCodecInfo.H264_CONSTRAINED_BASELINE_3_1, this.maxBframes));
+            // END:RTC_ENABLE_BFRAME
             break;
           default:
             Logging.w(TAG, "Unknown profile level id: " + profileLevelId);
         }
       }
+      else if (codecType == VideoCodecMimeType.H265) {
+        // START:RTC_ENABLE_BFRAME
+        if (this.bframesEnabled == true) {
+            format.setInteger(MediaFormat.KEY_MAX_B_FRAMES, this.maxBframes);       
+        }
+        // END:RTC_ENABLE_BFRAME
+      }
+
 
       if (codecName.equals("c2.google.av1.encoder")) {
         // Enable RTC mode in AV1 HW encoder.
@@ -339,7 +381,9 @@ class HardwareVideoEncoder implements VideoEncoder {
       textureInputSurface = null;
     }
     outputBuilders.clear();
-
+    //START:RTC_ENABLE_BFRAME
+    reorderTimestamp.clear();
+    //END:RTC_ENABLE_BFRAME
     codec = null;
     outputThread = null;
 
@@ -388,6 +432,7 @@ class HardwareVideoEncoder implements VideoEncoder {
 
     EncodedImage.Builder builder = EncodedImage.builder()
                                        .setCaptureTimeNs(videoFrame.getTimestampNs())
+                                       .setDecodingTimestamp(videoFrame.getTimestampRtp())
                                        .setEncodedWidth(videoFrame.getBuffer().getWidth())
                                        .setEncodedHeight(videoFrame.getBuffer().getHeight())
                                        .setRotation(videoFrame.getRotation());
@@ -399,6 +444,9 @@ class HardwareVideoEncoder implements VideoEncoder {
         (long) (TimeUnit.SECONDS.toMicros(1) / bitrateAdjuster.getAdjustedFramerateFps());
     nextPresentationTimestampUs += frameDurationUs;
 
+    //START:RTC_ENABLE_BFRAME
+    reorderTimestamp.put(presentationTimestampUs, videoFrame.getTimestampRtp());
+    //END:RTC_ENABLE_BFRAME
     final VideoCodecStatus returnValue;
     if (useSurfaceMode) {
       returnValue = encodeTextureBuffer(videoFrame, presentationTimestampUs);
@@ -410,6 +458,10 @@ class HardwareVideoEncoder implements VideoEncoder {
     if (returnValue != VideoCodecStatus.OK) {
       // Keep the output builders in sync with buffers in the codec.
       outputBuilders.pollLast();
+
+      //START:RTC_ENABLE_BFRAME
+      reorderTimestamp.remove(presentationTimestampUs);
+      //END:RTC_ENABLE_BFRAME
     }
 
     return returnValue;
@@ -658,10 +710,39 @@ class HardwareVideoEncoder implements VideoEncoder {
       final EncodedImage.FrameType frameType = isKeyFrame ? EncodedImage.FrameType.VideoFrameKey
                                                           : EncodedImage.FrameType.VideoFrameDelta;
 
+       //START:RTC_ENABLE_BFRAME
+      // When Bframe is enabled, encoded frames are created out of order.
+      // This is to find the original image timestamp(rtp) that matches the encoded image.
+      long timestampRtp = reorderTimestamp.get(info.presentationTimeUs); 
+      reorderTimestamp.remove(info.presentationTimeUs);
+
+      // Prevent timestamp map from continuously increasing.
+      while(reorderTimestamp.size() >= MAX_TIMESTAMP_Q_SIZE)
+      {
+        Long minKey = null;
+        Long minValue = Long.MAX_VALUE;
+
+        for (Map.Entry<Long, Long> entry : reorderTimestamp.entrySet()) {
+            if (entry.getValue() < minValue) {
+                minKey = entry.getKey();
+                minValue = entry.getValue();
+            }
+        }
+        if(minKey != null)
+        {
+          reorderTimestamp.remove(minKey);
+        }
+      }
+      //END:RTC_ENABLE_BFRAME
+      
       EncodedImage.Builder builder = outputBuilders.poll();
       builder.setBuffer(frameBuffer, releaseCallback);
       builder.setFrameType(frameType);
       builder.setQp(qp);
+      //START:RTC_ENABLE_BFRAME
+      builder.setTimestampRtp(timestampRtp);
+      //END:RTC_ENABLE_BFRAME
+
 
       EncodedImage encodedImage = builder.createEncodedImage();
       // TODO(mellem):  Set codec-specific info.
diff --git a/sdk/android/src/java/org/webrtc/MediaCodecUtils.java b/sdk/android/src/java/org/webrtc/MediaCodecUtils.java
index 5417fec..2b96b9e 100644
--- a/sdk/android/src/java/org/webrtc/MediaCodecUtils.java
+++ b/sdk/android/src/java/org/webrtc/MediaCodecUtils.java
@@ -29,6 +29,9 @@ class MediaCodecUtils {
   static final String INTEL_PREFIX = "OMX.Intel.";
   static final String NVIDIA_PREFIX = "OMX.Nvidia.";
   static final String QCOM_PREFIX = "OMX.qcom.";
+  // Qualcomm codec name added in Android 10 and higher.
+  static final String QTI_PREFIX = "c2.qti.";
+  static final String C2_EXYNOS_PREFIX = "c2.exynos.";
   static final String[] SOFTWARE_IMPLEMENTATION_PREFIXES = {
       "OMX.google.", "OMX.SEC.", "c2.android"};
 
diff --git a/sdk/android/src/java/org/webrtc/MediaCodecVideoDecoderFactory.java b/sdk/android/src/java/org/webrtc/MediaCodecVideoDecoderFactory.java
index 9a73bc4..01316ae 100644
--- a/sdk/android/src/java/org/webrtc/MediaCodecVideoDecoderFactory.java
+++ b/sdk/android/src/java/org/webrtc/MediaCodecVideoDecoderFactory.java
@@ -12,7 +12,8 @@ package org.webrtc;
 
 import static org.webrtc.MediaCodecUtils.EXYNOS_PREFIX;
 import static org.webrtc.MediaCodecUtils.QCOM_PREFIX;
-
+import static org.webrtc.MediaCodecUtils.QTI_PREFIX;
+import static org.webrtc.MediaCodecUtils.C2_EXYNOS_PREFIX;
 import android.media.MediaCodecInfo;
 import android.media.MediaCodecInfo.CodecCapabilities;
 import android.media.MediaCodecList;
@@ -71,8 +72,22 @@ class MediaCodecVideoDecoderFactory implements VideoDecoderFactory {
       if (codec != null) {
         String name = type.name();
         if (type == VideoCodecMimeType.H264 && isH264HighProfileSupported(codec)) {
-          supportedCodecInfos.add(new VideoCodecInfo(
-              name, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ true)));
+          VideoCodecInfo info = new VideoCodecInfo(
+            name, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ true));
+          //START:RTC_ENABLE_BFRAME
+          // The decoder always supports B frames.
+          info.setBframeEnabled(true);
+          //END:RTC_ENABLE_BFRAME
+          supportedCodecInfos.add(info);
+        }
+        if (type == VideoCodecMimeType.H265) {
+          VideoCodecInfo info = new VideoCodecInfo(
+            name, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ true));
+          //START:RTC_ENABLE_BFRAME
+          // The decoder always supports B frames.
+          info.setBframeEnabled(true);
+          //END:RTC_ENABLE_BFRAME
+          supportedCodecInfos.add(info);
         }
 
         supportedCodecInfos.add(new VideoCodecInfo(
@@ -128,7 +143,7 @@ class MediaCodecVideoDecoderFactory implements VideoDecoderFactory {
   private boolean isH264HighProfileSupported(MediaCodecInfo info) {
     String name = info.getName();
     // Support H.264 HP decoding on QCOM chips.
-    if (name.startsWith(QCOM_PREFIX)) {
+    if (name.startsWith(QCOM_PREFIX) || name.startsWith(QTI_PREFIX) || name.startsWith(C2_EXYNOS_PREFIX)) {
       return true;
     }
     // Support H.264 HP decoding on Exynos chips for Android M and above.
diff --git a/sdk/android/src/jni/encoded_image.cc b/sdk/android/src/jni/encoded_image.cc
index 9bd73a4..0add8d6 100644
--- a/sdk/android/src/jni/encoded_image.cc
+++ b/sdk/android/src/jni/encoded_image.cc
@@ -63,13 +63,19 @@ ScopedJavaLocalRef<jobject> NativeToJavaEncodedImage(
     qp = NativeToJavaInteger(jni, image.qp_);
   // TODO(bugs.webrtc.org/9378): Keep a reference to the C++ EncodedImage data,
   // and use the releaseCallback to manage lifetime.
-  return Java_EncodedImage_Constructor(
+  ScopedJavaLocalRef<jobject> encoded_image = Java_EncodedImage_Constructor(
       jni, buffer,
       /*releaseCallback=*/ScopedJavaGlobalRef<jobject>(nullptr),
       static_cast<int>(image._encodedWidth),
       static_cast<int>(image._encodedHeight),
       image.capture_time_ms_ * rtc::kNumNanosecsPerMillisec, frame_type,
       static_cast<jint>(image.rotation_), qp);
+
+    //START:RTC_ENABLE_BFRAME
+    Java_EncodedImage_setTimestampRtp(jni, encoded_image, static_cast<long>(image.RtpTimestamp()));
+    //END:RTC_ENABLE_BFRAME
+
+  return encoded_image;
 }
 
 ScopedJavaLocalRef<jobjectArray> NativeToJavaFrameTypeArray(
@@ -113,5 +119,18 @@ int64_t GetJavaEncodedImageCaptureTimeNs(
   return Java_EncodedImage_getCaptureTimeNs(env, j_encoded_image);
 }
 
+//START:RTC_ENABLE_BFRAME
+uint32_t GetJavaEncodedImageTimestampRtp(
+        JNIEnv* env,
+        const JavaRef<jobject>& j_encoded_image) {
+    return Java_EncodedImage_getTimestampRtp(env, j_encoded_image);
+}
+
+uint32_t GetJavaEncodedImageDecodingTimestamp(
+          JNIEnv* env,
+        const JavaRef<jobject>& j_encoded_image) {
+    return Java_EncodedImage_getDecodingTimestamp(env, j_encoded_image);
+//END:RTC_ENABLE_BFRAME    
+}
 }  // namespace jni
 }  // namespace webrtc
diff --git a/sdk/android/src/jni/encoded_image.h b/sdk/android/src/jni/encoded_image.h
index 2e89286..d6af4c7 100644
--- a/sdk/android/src/jni/encoded_image.h
+++ b/sdk/android/src/jni/encoded_image.h
@@ -39,6 +39,17 @@ int64_t GetJavaEncodedImageCaptureTimeNs(
     JNIEnv* jni,
     const JavaRef<jobject>& j_encoded_image);
 
+
+//START:RTC_ENABLE_BFRAME
+uint32_t GetJavaEncodedImageTimestampRtp(
+        JNIEnv* env,
+        const JavaRef<jobject>& j_encoded_image);
+
+uint32_t GetJavaEncodedImageDecodingTimestamp(
+          JNIEnv* env,
+        const JavaRef<jobject>& j_encoded_image);
+//END:RTC_ENABLE_BFRAME
+
 }  // namespace jni
 }  // namespace webrtc
 
diff --git a/sdk/android/src/jni/video_codec_info.cc b/sdk/android/src/jni/video_codec_info.cc
index 42e7b5d..976a2ed 100644
--- a/sdk/android/src/jni/video_codec_info.cc
+++ b/sdk/android/src/jni/video_codec_info.cc
@@ -15,6 +15,7 @@
 #include "sdk/android/src/jni/jni_helpers.h"
 
 #include "absl/container/inlined_vector.h"
+#include "rtc_base/logging.h"
 
 namespace webrtc {
 namespace jni {
@@ -26,10 +27,19 @@ SdpVideoFormat VideoCodecInfoToSdpVideoFormat(JNIEnv* jni,
   for (const auto& scalabilityMode : javaScalabilityModes) {
     scalabilityModes.push_back(static_cast<webrtc::ScalabilityMode>(scalabilityMode));
   }
-  return SdpVideoFormat(
+  
+  SdpVideoFormat format = SdpVideoFormat(
       JavaToNativeString(jni, Java_VideoCodecInfo_getName(jni, j_info)),
       JavaToNativeStringMap(jni, Java_VideoCodecInfo_getParams(jni, j_info)),
       scalabilityModes);
+
+  //START:RTC_ENABLE_BFRAME
+  [[maybe_unused]] bool bframe_enabled = (Java_VideoCodecInfo_getBframeEnabled(jni, j_info) != JNI_FALSE)?true:false;
+  format.bframe_enabled = bframe_enabled;
+  RTC_LOG(LS_VERBOSE) << "VideoCodecInfoToSdpVideoFormat. name:" << format.name.c_str() << ", bframe_enaled:" << (format.bframe_enabled?"True":"False");  
+  //END:RTC_ENABLE_BFRAME
+
+  return format;
 }
 
 ScopedJavaLocalRef<jobject> SdpVideoFormatToVideoCodecInfo(
@@ -48,6 +58,11 @@ ScopedJavaLocalRef<jobject> SdpVideoFormatToVideoCodecInfo(
   }
   Java_VideoCodecInfo_setScalabilityModes(jni, codec, NativeToJavaIntArray(jni, temp));
 
+  //START:RTC_ENABLE_BFRAME
+  RTC_LOG(LS_VERBOSE) << "SdpVideoFormatToVideoCodecInfo. name:" << format.name.c_str() << ", bframe_enaled:" << (format.bframe_enabled?"True":"False");
+  Java_VideoCodecInfo_setBframeEnabled(jni, codec, format.bframe_enabled?JNI_TRUE:JNI_FALSE);
+  //START:RTC_ENABLE_BFRAME
+
   return codec;
 }
 
diff --git a/sdk/android/src/jni/video_decoder_wrapper.cc b/sdk/android/src/jni/video_decoder_wrapper.cc
index e083df5..9b75a39 100644
--- a/sdk/android/src/jni/video_decoder_wrapper.cc
+++ b/sdk/android/src/jni/video_decoder_wrapper.cc
@@ -109,10 +109,32 @@ int32_t VideoDecoderWrapper::Decode(const EncodedImage& image_param,
   frame_extra_info.timestamp_ntp = input_image.ntp_time_ms_;
   frame_extra_info.qp =
       qp_parsing_enabled_ ? ParseQP(input_image) : absl::nullopt;
+
+#ifdef RTC_ENABLE_BFRAME    
   {
+    // It already has been set by the native
+    frame_extra_info.decoding_timestamp = image_param.Dts().value_or(image_param.RtpTimestamp());
+    frame_extra_info.is_bframe = input_image.IsBFrame().value_or(false);
+
     MutexLock lock(&frame_extra_infos_lock_);
-    frame_extra_infos_.push_back(frame_extra_info);
+    frame_extra_infos_.insert(std::make_pair(frame_extra_info.timestamp_ns, frame_extra_info));
+    // In order for timestamp_ns to be created sequentially when calling OnDecodedFrame, 
+    // Presestation Timestamp must be calculated using CompositionTimestamp.
+    // PTS(capture_time_ms_) = CTS(CompositionTimestamp) * 90 + DTS(RtpTimestamp)
+    RTC_LOG(LS_VERBOSE)  << "Decode." 
+                    << "  timestamp_ns:" << frame_extra_info.timestamp_ns
+                    << " capture:" << input_image.capture_time_ms_ 
+                    << ", pts:" << image_param.RtpTimestamp() 
+                    << ", dts:" << image_param.Dts().value_or(0)
+                    << ", key:" << image_param.FrameType()
+                    << ", bframe: " << image_param.IsBFrame().value_or(false);
   }
+#else
+  {
+    MutexLock lock(&frame_extra_infos_lock_);
+    frame_extra_infos_.push_back(frame_extra_info);
+  } 
+#endif
 
   JNIEnv* env = AttachCurrentThreadIfNeeded();
   ScopedJavaLocalRef<jobject> jinput_image =
@@ -165,6 +187,35 @@ void VideoDecoderWrapper::OnDecodedFrame(
   const int64_t timestamp_ns = GetJavaVideoFrameTimestampNs(env, j_frame);
 
   FrameExtraInfo frame_extra_info;
+#ifdef RTC_ENABLE_BFRAME  
+  {
+    MutexLock lock(&frame_extra_infos_lock_);
+
+    auto it = frame_extra_infos_.find(timestamp_ns);
+    if(it == frame_extra_infos_.end())
+    {
+      RTC_LOG(LS_WARNING)
+          << "Java decoder produced an unexpected frame: " << timestamp_ns;
+      return;
+    }
+    frame_extra_info = it->second;
+    frame_extra_infos_.erase(it);
+    uint32_t pts = frame_extra_info.timestamp_rtp;
+    RTC_LOG(LS_VERBOSE)  << "Decoded." 
+                      << " timestamp_ns:" << timestamp_ns
+                      << ", pts:" << pts
+                      << ", decoded_time_ms: " << JavaToNativeOptionalInt(env, j_decode_time_ms).value_or(-1)
+                      << ". queue: " << frame_extra_infos_.size();
+
+    const auto count = std::erase_if(frame_extra_infos_, [pts](const auto& item) {
+        auto const& [timestamp_ns, frame_extra_info] = item;
+        return frame_extra_info.timestamp_rtp < pts;
+    });
+    if(count != 0) {
+        RTC_LOG(LS_WARNING) << "Removes " << count << "  dropped FrameExtraInfo.";
+    }
+  }
+#else
   {
     MutexLock lock(&frame_extra_infos_lock_);
 
@@ -181,13 +232,18 @@ void VideoDecoderWrapper::OnDecodedFrame(
       // find a matching timestamp.
     } while (frame_extra_info.timestamp_ns != timestamp_ns);
   }
+#endif
 
   VideoFrame frame =
       JavaToNativeFrame(env, j_frame, frame_extra_info.timestamp_rtp);
   frame.set_ntp_time_ms(frame_extra_info.timestamp_ntp);
 
+#ifdef RTC_ENABLE_BFRAME  
+  absl::optional<int32_t> decoding_time_ms = -1;
+#else
   absl::optional<int32_t> decoding_time_ms =
       JavaToNativeOptionalInt(env, j_decode_time_ms);
+#endif
 
   absl::optional<uint8_t> decoder_qp =
       cast_optional<uint8_t, int32_t>(JavaToNativeOptionalInt(env, j_qp));
diff --git a/sdk/android/src/jni/video_decoder_wrapper.h b/sdk/android/src/jni/video_decoder_wrapper.h
index 53246f3..04a2f52 100644
--- a/sdk/android/src/jni/video_decoder_wrapper.h
+++ b/sdk/android/src/jni/video_decoder_wrapper.h
@@ -67,6 +67,11 @@ class VideoDecoderWrapper : public VideoDecoder {
     int64_t timestamp_ntp;
     absl::optional<uint8_t> qp;
 
+#ifdef RTC_ENABLE_BFRAME
+    uint32_t decoding_timestamp;
+    bool is_bframe = false;
+#endif
+
     FrameExtraInfo();
     FrameExtraInfo(const FrameExtraInfo&);
     ~FrameExtraInfo();
@@ -109,8 +114,13 @@ class VideoDecoderWrapper : public VideoDecoder {
   // Accessed both on the decoder thread and the callback thread.
   std::atomic<bool> qp_parsing_enabled_;
   Mutex frame_extra_infos_lock_;
+#ifdef RTC_ENABLE_BFRAME
+  std::map<int64_t, FrameExtraInfo> frame_extra_infos_
+      RTC_GUARDED_BY(frame_extra_infos_lock_);    
+#else
   std::deque<FrameExtraInfo> frame_extra_infos_
-      RTC_GUARDED_BY(frame_extra_infos_lock_);
+      RTC_GUARDED_BY(frame_extra_infos_lock_);  
+#endif      
 };
 
 /* If the j_decoder is a wrapped native decoder, unwrap it. If it is not,
diff --git a/sdk/android/src/jni/video_encoder_wrapper.cc b/sdk/android/src/jni/video_encoder_wrapper.cc
index ace53c9..9c2573e 100644
--- a/sdk/android/src/jni/video_encoder_wrapper.cc
+++ b/sdk/android/src/jni/video_encoder_wrapper.cc
@@ -13,6 +13,7 @@
 #include <utility>
 
 #include "common_video/h264/h264_common.h"
+#include "common_video/h265/h265_common.h"
 #include "modules/video_coding/include/video_codec_interface.h"
 #include "modules/video_coding/include/video_error_codes.h"
 #include "modules/video_coding/svc/scalable_video_controller_no_layering.h"
@@ -69,6 +70,12 @@ int32_t VideoEncoderWrapper::InitEncodeInternal(JNIEnv* jni) {
       automatic_resize_on = true;
   }
 
+  bool bframes_enabled = false;
+#ifdef RTC_ENABLE_BFRAME
+  bframes_enabled = IsBFrameEnabled();
+  RTC_LOG(LS_INFO) << "VideoEncoderWrapper::InitEncodeInternal bframes_enabled:" << (bframes_enabled?"True":"False"); 
+#endif
+  
   RTC_DCHECK(capabilities_);
   ScopedJavaLocalRef<jobject> capabilities =
       Java_Capabilities_Constructor(jni, capabilities_->loss_notification);
@@ -78,7 +85,9 @@ int32_t VideoEncoderWrapper::InitEncodeInternal(JNIEnv* jni) {
       static_cast<int>(codec_settings_.startBitrate),
       static_cast<int>(codec_settings_.maxFramerate),
       static_cast<int>(codec_settings_.numberOfSimulcastStreams),
-      automatic_resize_on, capabilities);
+      automatic_resize_on, capabilities, 
+      static_cast<bool>(bframes_enabled)
+      );
 
   ScopedJavaLocalRef<jobject> callback =
       Java_VideoEncoderWrapper_createEncoderCallback(jni,
@@ -131,10 +140,12 @@ int32_t VideoEncoderWrapper::Release() {
   int32_t status = JavaToNativeVideoCodecStatus(
       jni, Java_VideoEncoder_release(jni, encoder_));
   RTC_LOG(LS_INFO) << "release: " << status;
+#ifndef RTC_ENABLE_BFRAME  
   {
     MutexLock lock(&frame_extra_infos_lock_);
     frame_extra_infos_.clear();
   }
+#endif  
   initialized_ = false;
 
   return status;
@@ -161,6 +172,7 @@ int32_t VideoEncoderWrapper::Encode(
   ScopedJavaLocalRef<jobject> encode_info =
       Java_EncodeInfo_Constructor(jni, j_frame_types);
 
+#ifndef RTC_ENABLE_BFRAME
   FrameExtraInfo info;
   info.capture_time_ns = frame.timestamp_us() * rtc::kNumNanosecsPerMicrosec;
   info.timestamp_rtp = frame.timestamp();
@@ -168,6 +180,7 @@ int32_t VideoEncoderWrapper::Encode(
     MutexLock lock(&frame_extra_infos_lock_);
     frame_extra_infos_.push_back(info);
   }
+#endif  
 
   ScopedJavaLocalRef<jobject> j_frame = NativeToJavaVideoFrame(jni, frame);
   ScopedJavaLocalRef<jobject> ret =
@@ -267,7 +280,15 @@ void VideoEncoderWrapper::OnEncodedFrame(
   EncodedImage frame = JavaToNativeEncodedImage(jni, j_encoded_image);
   int64_t capture_time_ns =
       GetJavaEncodedImageCaptureTimeNs(jni, j_encoded_image);
+  //START:RTC_ENABLE_BFRAME      
+  [[maybe_unused]] uint32_t timestamp_rtp = 
+      GetJavaEncodedImageTimestampRtp(jni, j_encoded_image);
+
+  [[maybe_unused]] uint32_t decoding_timestamp = 
+      GetJavaEncodedImageDecodingTimestamp(jni, j_encoded_image);
+  //END:RTC_ENABLE_BFRAME      
 
+#ifndef RTC_ENABLE_BFRAME
   // Encoded frames are delivered in the order received, but some of them
   // may be dropped, so remove records of frames older than the current
   // one.
@@ -296,6 +317,7 @@ void VideoEncoderWrapper::OnEncodedFrame(
     frame_extra_info = frame_extra_infos_.front();
     frame_extra_infos_.pop_front();
   }
+#endif
 
   // This is a bit subtle. The `frame` variable from the lambda capture is
   // const. Which implies that (i) we need to make a copy to be able to
@@ -305,12 +327,27 @@ void VideoEncoderWrapper::OnEncodedFrame(
   // CopyOnWriteBuffer.
   EncodedImage frame_copy = frame;
 
-  frame_copy.SetRtpTimestamp(frame_extra_info.timestamp_rtp);
   frame_copy.capture_time_ms_ = capture_time_ns / rtc::kNumNanosecsPerMillisec;
 
   if (frame_copy.qp_ < 0)
     frame_copy.qp_ = ParseQp(frame);
 
+#ifdef RTC_ENABLE_BFRAME
+  frame_copy.SetRtpTimestamp(timestamp_rtp);
+  frame_copy.SetDts(decoding_timestamp);
+  frame_copy.SetBFrame(IsBFrame());  
+
+  RTC_LOG(LS_VERBOSE) << "Encoded." 
+                    << " capture: " << frame_copy.capture_time_ms_
+                    << " pts:" << timestamp_rtp 
+                    << " dts:" << decoding_timestamp
+                    << "(" << ((double)timestamp_rtp - (double)decoding_timestamp) << ")"
+                    << " slice:" << GetSliceType().value_or(-1)
+                    << " bframe:" << IsBFrame();
+#else
+  frame_copy.SetRtpTimestamp(frame_extra_info.timestamp_rtp);
+#endif
+
   CodecSpecificInfo info(ParseCodecSpecificInfo(frame));
 
   if (callback_ != nullptr) {
@@ -373,6 +410,45 @@ int VideoEncoderWrapper::ParseQp(rtc::ArrayView<const uint8_t> buffer) {
   return success ? qp : -1;  // -1 means unknown QP.
 }
 
+#ifdef RTC_ENABLE_BFRAME
+// It must be called after ParseQp.
+absl::optional<uint8_t> VideoEncoderWrapper::GetSliceType() const {
+  switch (codec_settings_.codecType) {
+    case kVideoCodecH264:
+      return h264_bitstream_parser_.GetLastSliceType();
+    case kVideoCodecH265:
+      return h265_bitstream_parser_.GetLastSliceType();
+    default:
+      return 0xFF;
+  }
+}
+
+bool VideoEncoderWrapper::IsBFrame() const {
+  absl::optional<uint8_t> slice_type;
+  switch (codec_settings_.codecType) {
+    case kVideoCodecH264:
+      slice_type = GetSliceType();
+      return (slice_type.has_value() && slice_type.value() == H264::SliceType::kB);
+    case kVideoCodecH265:
+      slice_type = GetSliceType();
+      return (slice_type.has_value() && slice_type.value() == H265::SliceType::kB);
+    default:
+      return false;
+  }
+}
+
+bool VideoEncoderWrapper::IsBFrameEnabled() const {
+ switch (codec_settings_.codecType) {
+    case kVideoCodecH264:
+      return codec_settings_.H264().bframe_enabled;
+    case kVideoCodecH265:
+      return codec_settings_.H265().bframe_enabled;
+    default:
+      return false;
+  }
+}
+#endif
+
 CodecSpecificInfo VideoEncoderWrapper::ParseCodecSpecificInfo(
     const EncodedImage& frame) {
   const bool key_frame = frame._frameType == VideoFrameType::kVideoFrameKey;
diff --git a/sdk/android/src/jni/video_encoder_wrapper.h b/sdk/android/src/jni/video_encoder_wrapper.h
index 04d70f3..3d0df8f 100644
--- a/sdk/android/src/jni/video_encoder_wrapper.h
+++ b/sdk/android/src/jni/video_encoder_wrapper.h
@@ -73,6 +73,13 @@ class VideoEncoderWrapper : public VideoEncoder {
 
   int ParseQp(rtc::ArrayView<const uint8_t> buffer);
 
+#ifdef RTC_ENABLE_BFRAME
+  bool IsBFrameEnabled() const;
+  // // It must be called after ParseQp.
+  absl::optional<uint8_t> GetSliceType() const;
+  bool IsBFrame() const;
+#endif
+
   CodecSpecificInfo ParseCodecSpecificInfo(const EncodedImage& frame);
 
   ScopedJavaLocalRef<jobject> ToJavaBitrateAllocation(
@@ -94,10 +101,12 @@ class VideoEncoderWrapper : public VideoEncoder {
   const ScopedJavaGlobalRef<jobject> encoder_;
   const ScopedJavaGlobalRef<jclass> int_array_class_;
 
+#ifndef RTC_ENABLE_BFRAME
   // Modified both on the encoder thread and the callback thread.
   Mutex frame_extra_infos_lock_;
   std::deque<FrameExtraInfo> frame_extra_infos_
       RTC_GUARDED_BY(frame_extra_infos_lock_);
+#endif
   EncodedImageCallback* callback_;
   bool initialized_;
   int num_resets_;
diff --git a/sdk/android/src/jni/video_frame.cc b/sdk/android/src/jni/video_frame.cc
index 121b34f..0b0962c 100644
--- a/sdk/android/src/jni/video_frame.cc
+++ b/sdk/android/src/jni/video_frame.cc
@@ -175,11 +175,20 @@ AndroidVideoI420Buffer::~AndroidVideoI420Buffer() {
 
 }  // namespace
 
+//START:RTC_ENABLE_BFRAME
 int64_t GetJavaVideoFrameTimestampNs(JNIEnv* jni,
                                      const JavaRef<jobject>& j_video_frame) {
   return Java_VideoFrame_getTimestampNs(jni, j_video_frame);
 }
 
+uint32_t GetJavaVideoFrameTimestampRtp(JNIEnv* jni,
+                                     const JavaRef<jobject>& j_video_frame) {
+
+  return Java_VideoFrame_getTimestampRtp(jni, j_video_frame);
+}
+//END:RTC_ENABLE_BFRAME
+
+
 rtc::scoped_refptr<AndroidVideoBuffer> AndroidVideoBuffer::Adopt(
     JNIEnv* jni,
     const JavaRef<jobject>& j_video_frame_buffer) {
@@ -298,16 +307,26 @@ ScopedJavaLocalRef<jobject> NativeToJavaVideoFrame(JNIEnv* jni,
     ScopedJavaLocalRef<jobject> j_video_frame_buffer(
         jni, android_buffer->video_frame_buffer());
     Java_Buffer_retain(jni, j_video_frame_buffer);
-    return Java_VideoFrame_Constructor(
+    ScopedJavaLocalRef<jobject> j_video_frame = Java_VideoFrame_Constructor(
         jni, j_video_frame_buffer, static_cast<jint>(frame.rotation()),
         static_cast<jlong>(frame.timestamp_us() *
                            rtc::kNumNanosecsPerMicrosec));
+    //START:RTC_ENABLE_BFRAME
+    // TODO: Improved so that parameters can be added to the constructor. 
+    Java_VideoFrame_setTimestampRtp(jni, j_video_frame, static_cast<jlong>(frame.timestamp()));
+    //END:RTC_ENABLE_BFRAME
+    return j_video_frame;
   } else {
-    return Java_VideoFrame_Constructor(
+    ScopedJavaLocalRef<jobject> j_video_frame =  Java_VideoFrame_Constructor(
         jni, WrapI420Buffer(jni, buffer->ToI420()),
         static_cast<jint>(frame.rotation()),
         static_cast<jlong>(frame.timestamp_us() *
                            rtc::kNumNanosecsPerMicrosec));
+    //START:RTC_ENABLE_BFRAME
+    // TODO: Improved so that parameters can be added to the constructor. 
+    //END:RTC_ENABLE_BFRAME
+    Java_VideoFrame_setTimestampRtp(jni, j_video_frame, static_cast<jlong>(frame.timestamp()));
+    return j_video_frame;                           
   }
 }
 
diff --git a/sdk/android/src/jni/video_frame.h b/sdk/android/src/jni/video_frame.h
index 9b916de..d4be1d1 100644
--- a/sdk/android/src/jni/video_frame.h
+++ b/sdk/android/src/jni/video_frame.h
@@ -37,6 +37,9 @@ void ReleaseJavaVideoFrame(JNIEnv* jni, const JavaRef<jobject>& j_video_frame);
 int64_t GetJavaVideoFrameTimestampNs(JNIEnv* jni,
                                      const JavaRef<jobject>& j_video_frame);
 
+uint32_t GetJavaVideoFrameTimestampRtp(JNIEnv* jni,
+                                     const JavaRef<jobject>& j_video_frame);
+
 }  // namespace jni
 }  // namespace webrtc
 
diff --git a/sdk/android/tests/src/org/webrtc/HardwareVideoEncoderTest.java b/sdk/android/tests/src/org/webrtc/HardwareVideoEncoderTest.java
index 3c24e00..89e484c 100644
--- a/sdk/android/tests/src/org/webrtc/HardwareVideoEncoderTest.java
+++ b/sdk/android/tests/src/org/webrtc/HardwareVideoEncoderTest.java
@@ -75,7 +75,8 @@ public class HardwareVideoEncoderTest {
       /* maxFramerate= */ 30,
       /* numberOfSimulcastStreams= */ 1,
       /* automaticResizeOn= */ true,
-      /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */));
+      /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */), 
+      /* bFramesEnabled */false);
   private static final long POLL_DELAY_MS = 10;
   private static final long DELIVER_ENCODED_IMAGE_DELAY_MS = 10;
   private static final EncodeInfo ENCODE_INFO_KEY_FRAME =
@@ -90,11 +91,11 @@ public class HardwareVideoEncoderTest {
 
     TestEncoder(MediaCodecWrapperFactory mediaCodecWrapperFactory, String codecName,
         VideoCodecMimeType codecType, Integer surfaceColorFormat, Integer yuvColorFormat,
-        Map<String, String> params, int keyFrameIntervalSec, int forceKeyFrameIntervalMs,
+        Map<String, String> params, int keyFrameIntervalSec, int forceKeyFrameIntervalMs, int maxBFrames,
         BitrateAdjuster bitrateAdjuster, EglBase14.Context sharedContext,
         boolean isEncodingStatisticsSupported) {
       super(mediaCodecWrapperFactory, codecName, codecType, surfaceColorFormat, yuvColorFormat,
-          params, keyFrameIntervalSec, forceKeyFrameIntervalMs, bitrateAdjuster, sharedContext);
+          params, keyFrameIntervalSec, forceKeyFrameIntervalMs, maxBFrames, bitrateAdjuster, sharedContext);
       this.isEncodingStatisticsSupported = isEncodingStatisticsSupported;
     }
 
@@ -179,7 +180,9 @@ public class HardwareVideoEncoderTest {
           /* surfaceColorFormat= */ null, colorFormat,
           /* params= */ new HashMap<>(),
           /* keyFrameIntervalSec= */ 0,
-          /* forceKeyFrameIntervalMs= */ 0, bitrateAdjuster,
+          /* forceKeyFrameIntervalMs= */ 0,
+          /* maxBFrames */ 0,
+          bitrateAdjuster,
           /* sharedContext= */ null, isEncodingStatisticsSupported);
     }
   }
@@ -421,7 +424,8 @@ public class HardwareVideoEncoderTest {
             /* maxFramerate= */ 15,
             /* numberOfSimulcastStreams= */ 1,
             /* automaticResizeOn= */ true,
-            /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */)),
+            /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */),
+            /* maxBFrames */ false),
         mockEncoderCallback);
 
     MediaFormat mediaFormat = fakeMediaCodecWrapper.getConfiguredFormat();
