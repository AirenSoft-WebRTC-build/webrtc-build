diff --git a/sdk/android/api/org/webrtc/EncodedImage.java b/sdk/android/api/org/webrtc/EncodedImage.java
index a6eef67..6896205 100644
--- a/sdk/android/api/org/webrtc/EncodedImage.java
+++ b/sdk/android/api/org/webrtc/EncodedImage.java
@@ -56,6 +56,31 @@ public class EncodedImage implements RefCounted {
   public final int rotation;
   public final @Nullable Integer qp;
 
+  //START:RTC_ENABLE_BFRAME
+  public long timestampRtp;
+  public long decodingTimestamp;
+  
+  @CalledByNative
+  public void setTimestampRtp(long timestampRtp) {
+    this.timestampRtp = timestampRtp;
+  }
+
+  @CalledByNative
+  public long getTimestampRtp() {
+    return this.timestampRtp;
+  }
+
+  public void setDecodingTimestamp(long timestamp) {
+    this.decodingTimestamp = timestamp;
+  }
+
+  @CalledByNative
+  public long getDecodingTimestamp() {
+    return this.decodingTimestamp;
+  }
+  //END:RTC_ENABLE_BFRAME
+
+
   // TODO(bugs.webrtc.org/9378): Use retain and release from jni code.
   @Override
   public void retain() {
@@ -130,7 +155,6 @@ public class EncodedImage implements RefCounted {
     private EncodedImage.FrameType frameType;
     private int rotation;
     private @Nullable Integer qp;
-
     private Builder() {}
 
     public Builder setBuffer(ByteBuffer buffer, @Nullable Runnable releaseCallback) {
@@ -175,9 +199,34 @@ public class EncodedImage implements RefCounted {
       return this;
     }
 
+    //START:RTC_ENABLE_BFRAME
+    private long timestampRtp;
+    public Builder setTimestampRtp(long timestampRtp) {
+      this.timestampRtp = timestampRtp;
+      return this;
+    }
+    public long getTimestampRtp() {
+      return this.timestampRtp;
+    }
+    private long decodingTimestamp;
+
+    public Builder setDecodingTimestamp(long decodingTimestamp) {
+      this.decodingTimestamp = decodingTimestamp;
+      return this;
+    }
+    //END:RTC_ENABLE_BFRAME
+
     public EncodedImage createEncodedImage() {
-      return new EncodedImage(buffer, releaseCallback, encodedWidth, encodedHeight, captureTimeNs,
-          frameType, rotation, qp);
+
+      EncodedImage encodedImage =  new EncodedImage(buffer, releaseCallback, encodedWidth, 
+        encodedHeight, captureTimeNs,frameType, rotation, qp);
+
+      //START:RTC_ENABLE_BFRAME
+      encodedImage.setTimestampRtp(timestampRtp);
+      encodedImage.setDecodingTimestamp(decodingTimestamp);
+      //END:RTC_ENABLE_BFRAME
+
+      return encodedImage;
     }
   }
 }
diff --git a/sdk/android/api/org/webrtc/HardwareVideoEncoderFactory.java b/sdk/android/api/org/webrtc/HardwareVideoEncoderFactory.java
index 8cf6714..dca617e 100644
--- a/sdk/android/api/org/webrtc/HardwareVideoEncoderFactory.java
+++ b/sdk/android/api/org/webrtc/HardwareVideoEncoderFactory.java
@@ -13,6 +13,9 @@ package org.webrtc;
 import static org.webrtc.MediaCodecUtils.EXYNOS_PREFIX;
 import static org.webrtc.MediaCodecUtils.INTEL_PREFIX;
 import static org.webrtc.MediaCodecUtils.QCOM_PREFIX;
+import static org.webrtc.MediaCodecUtils.QTI_PREFIX;
+import static org.webrtc.MediaCodecUtils.C2_EXYNOS_PREFIX;
+
 
 import android.media.MediaCodecInfo;
 import android.media.MediaCodecList;
@@ -86,6 +89,42 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
     this.codecAllowedPredicate = codecAllowedPredicate;
   }
 
+  //START:RTC_ENABLE_BFRAME
+  private int maxBframes;
+
+  /**
+   * Creates a HardwareVideoEncoderFactory that supports surface texture encoding.
+   *
+   * @param sharedContext The textures generated will be accessible from this context. May be null,
+   *                      this disables texture support.
+   * @param enableIntelVp8Encoder true if Intel's VP8 encoder enabled.
+   * @param enableH264HighProfile true if H264 High Profile enabled.
+   * @param maxBframes 0 is disabled, 1 or more activates B frames and specifies the number.
+   * @param codecAllowedPredicate optional predicate to filter codecs. All codecs are allowed
+   *                              when predicate is not provided.
+   */  
+  public HardwareVideoEncoderFactory(
+      EglBase.Context sharedContext, boolean enableIntelVp8Encoder, boolean enableH264HighProfile, int maxBframes) {
+    this(sharedContext, enableIntelVp8Encoder, enableH264HighProfile, maxBframes, /* codecAllowedPredicate= */ null);
+  }
+
+  public HardwareVideoEncoderFactory(EglBase.Context sharedContext, boolean enableIntelVp8Encoder,
+  boolean enableH264HighProfile, int maxBframes, @Nullable Predicate<MediaCodecInfo> codecAllowedPredicate) {
+    // Texture mode requires EglBase14.
+    if (sharedContext instanceof EglBase14.Context) {
+      this.sharedContext = (EglBase14.Context) sharedContext;
+    } else {
+      Logging.w(TAG, "No shared EglBase.Context.  Encoders will not use texture mode.");
+      this.sharedContext = null;
+    }
+    this.enableIntelVp8Encoder = enableIntelVp8Encoder;
+    this.enableH264HighProfile = enableH264HighProfile;
+    this.codecAllowedPredicate = codecAllowedPredicate;
+    this.maxBframes = maxBframes;
+  }
+  //END:RTC_ENABLE_BFRAME
+
+
   @Deprecated
   public HardwareVideoEncoderFactory(boolean enableIntelVp8Encoder, boolean enableH264HighProfile) {
     this(null, enableIntelVp8Encoder, enableH264HighProfile);
@@ -114,6 +153,11 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
       boolean isBaselineProfile = H264Utils.isSameH264Profile(
           input.params, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ false));
 
+      //START:RTC_ENABLE_BFRAME
+      Logging.d(TAG, "isHighProfile:" + isHighProfile + ", isBaselineProfile:" + isBaselineProfile + 
+        ", isH264HighProfileSupported:" +  isH264HighProfileSupported(info) + ", maxBFrames:" + this.maxBframes);
+      //END:RTC_ENABLE_BFRAME
+
       if (!isHighProfile && !isBaselineProfile) {
         return null;
       }
@@ -122,6 +166,14 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
       }
     }
 
+    //START:RTC_ENABLE_BFRAME
+    if (this.maxBframes > 0) {
+      return new HardwareVideoEncoder(new MediaCodecWrapperFactoryImpl(), codecName, type,
+          surfaceColorFormat, yuvColorFormat, input.params, PERIODIC_KEY_FRAME_INTERVAL_S,
+          getForcedKeyFrameIntervalMs(type, codecName), this.maxBframes, createBitrateAdjuster(type, codecName),
+          sharedContext);
+    }
+    //END:RTC_ENABLE_BFRAME        
     return new HardwareVideoEncoder(new MediaCodecWrapperFactoryImpl(), codecName, type,
         surfaceColorFormat, yuvColorFormat, input.params, PERIODIC_KEY_FRAME_INTERVAL_S,
         getForcedKeyFrameIntervalMs(type, codecName), createBitrateAdjuster(type, codecName),
@@ -130,6 +182,37 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
 
   @Override
   public VideoCodecInfo[] getSupportedCodecs() {
+    //START:RTC_ENABLE_BFRAME
+    if (this.maxBframes > 0)
+    {
+      List<VideoCodecInfo> supportedCodecInfos = new ArrayList<VideoCodecInfo>();
+      for (VideoCodecMimeType type :
+          new VideoCodecMimeType[] {VideoCodecMimeType.VP8, VideoCodecMimeType.VP9,
+              VideoCodecMimeType.H264, VideoCodecMimeType.AV1, VideoCodecMimeType.H265}) {
+        MediaCodecInfo codec = findCodecForType(type);
+        if (codec != null) {
+          String name = type.name();
+          if (type == VideoCodecMimeType.H264 && isH264HighProfileSupported(codec)) {
+            VideoCodecInfo info = new VideoCodecInfo(
+              name, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ true));
+            info.setBframeEnabled(true);
+            supportedCodecInfos.add(info);
+          } else if (type == VideoCodecMimeType.H265) {
+            VideoCodecInfo info = new VideoCodecInfo(
+              name, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ false));
+            info.setBframeEnabled(true);
+            supportedCodecInfos.add(info);
+            continue;
+          }
+          
+          supportedCodecInfos.add(new VideoCodecInfo(
+            name, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ false)));          
+        }
+      }
+      return supportedCodecInfos.toArray(new VideoCodecInfo[supportedCodecInfos.size()]);
+    }
+    //END:RTC_ENABLE_BFRAME
+
     List<VideoCodecInfo> supportedCodecInfos = new ArrayList<VideoCodecInfo>();
     // Generate a list of supported codecs in order of preference:
     // VP8, VP9, H264 (high profile), H264 (baseline profile), AV1 and H265.
@@ -278,6 +361,9 @@ public class HardwareVideoEncoderFactory implements VideoEncoderFactory {
 
   private boolean isH264HighProfileSupported(MediaCodecInfo info) {
     return enableH264HighProfile && Build.VERSION.SDK_INT > Build.VERSION_CODES.M
-        && info.getName().startsWith(EXYNOS_PREFIX);
+        && (info.getName().startsWith(EXYNOS_PREFIX)
+            || info.getName().startsWith(QTI_PREFIX)
+            || info.getName().startsWith(QCOM_PREFIX)
+            || info.getName().startsWith(C2_EXYNOS_PREFIX));
   }
 }
diff --git a/sdk/android/api/org/webrtc/VideoCodecInfo.java b/sdk/android/api/org/webrtc/VideoCodecInfo.java
index 86d67d6..3a8c69a 100644
--- a/sdk/android/api/org/webrtc/VideoCodecInfo.java
+++ b/sdk/android/api/org/webrtc/VideoCodecInfo.java
@@ -37,6 +37,10 @@ public class VideoCodecInfo {
   public int[] scalabilityModes;
   @Deprecated public final int payload;
 
+  //START:RTC_ENABLE_BFRAME
+  public boolean bframeEnabled;
+  //END:RTC_ENABLE_BFRAME
+
   @CalledByNative
   public VideoCodecInfo(String name, Map<String, String> params) {
     this.payload = 0;
@@ -97,5 +101,15 @@ public class VideoCodecInfo {
     scalabilityModes = values;
   }
 
+  //START:RTC_ENABLE_BFRAME
+  @CalledByNative
+  boolean getBframeEnabled() {
+    return bframeEnabled;
+  }
 
+  @CalledByNative
+  void setBframeEnabled(boolean value) {
+    bframeEnabled = value;
+  }  
+  //END:RTC_ENABLE_BFRAME
 }
diff --git a/sdk/android/api/org/webrtc/VideoEncoder.java b/sdk/android/api/org/webrtc/VideoEncoder.java
index be62686..4aa761e 100644
--- a/sdk/android/api/org/webrtc/VideoEncoder.java
+++ b/sdk/android/api/org/webrtc/VideoEncoder.java
@@ -29,18 +29,27 @@ public interface VideoEncoder {
     public final int numberOfSimulcastStreams;
     public final boolean automaticResizeOn;
     public final Capabilities capabilities;
-
+    //START:RTC_ENABLE_BFRAME
+    public final boolean bframeEnabled;
+    //END:RTC_ENABLE_BFRAME
     // TODO(bugs.webrtc.org/10720): Remove.
     @Deprecated
     public Settings(int numberOfCores, int width, int height, int startBitrate, int maxFramerate,
         int numberOfSimulcastStreams, boolean automaticResizeOn) {
       this(numberOfCores, width, height, startBitrate, maxFramerate, numberOfSimulcastStreams,
-          automaticResizeOn, new VideoEncoder.Capabilities(false /* lossNotification */));
+          automaticResizeOn, new VideoEncoder.Capabilities(false /* lossNotification */), 
+          //START:RTC_ENABLE_BFRAME          
+          false);
+          //END:RTC_ENABLE_BFRAME
+
     }
 
     @CalledByNative("Settings")
     public Settings(int numberOfCores, int width, int height, int startBitrate, int maxFramerate,
-        int numberOfSimulcastStreams, boolean automaticResizeOn, Capabilities capabilities) {
+        int numberOfSimulcastStreams, boolean automaticResizeOn, Capabilities capabilities, 
+        //START:RTC_ENABLE_BFRAME          
+        boolean bframeEnabled) {
+        //END:RTC_ENABLE_BFRAME          
       this.numberOfCores = numberOfCores;
       this.width = width;
       this.height = height;
@@ -49,6 +58,9 @@ public interface VideoEncoder {
       this.numberOfSimulcastStreams = numberOfSimulcastStreams;
       this.automaticResizeOn = automaticResizeOn;
       this.capabilities = capabilities;
+      //START:RTC_ENABLE_BFRAME
+      this.bframeEnabled = bframeEnabled;
+      //END:RTC_ENABLE_BFRAME
     }
   }
 
diff --git a/sdk/android/api/org/webrtc/VideoFrame.java b/sdk/android/api/org/webrtc/VideoFrame.java
index 443a031..fd8fc4c 100644
--- a/sdk/android/api/org/webrtc/VideoFrame.java
+++ b/sdk/android/api/org/webrtc/VideoFrame.java
@@ -231,4 +231,20 @@ public class VideoFrame implements RefCounted {
   public void release() {
     buffer.release();
   }
+
+  //START:RTC_ENABLE_BFRAME
+  private long timestampRtp;
+
+  @CalledByNative
+  public void setTimestampRtp(long timestampRtp)
+  {
+    this.timestampRtp = timestampRtp;
+  }
+
+  @CalledByNative
+  public long getTimestampRtp()
+  {
+    return this.timestampRtp;
+  }
+  //END:RTC_ENABLE_BFRAME
 }
diff --git a/sdk/android/instrumentationtests/src/org/webrtc/AndroidVideoDecoderInstrumentationTest.java b/sdk/android/instrumentationtests/src/org/webrtc/AndroidVideoDecoderInstrumentationTest.java
index 6f44812..c160bbc 100644
--- a/sdk/android/instrumentationtests/src/org/webrtc/AndroidVideoDecoderInstrumentationTest.java
+++ b/sdk/android/instrumentationtests/src/org/webrtc/AndroidVideoDecoderInstrumentationTest.java
@@ -65,7 +65,10 @@ public final class AndroidVideoDecoderInstrumentationTest {
   private static final VideoEncoder.Settings ENCODER_SETTINGS =
       new VideoEncoder.Settings(1 /* core */, TEST_FRAME_WIDTH, TEST_FRAME_HEIGHT, 300 /* kbps */,
           30 /* fps */, 1 /* numberOfSimulcastStreams */, true /* automaticResizeOn */,
-          /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */));
+          /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */), 
+          //START:RTC_ENABLE_BFRAME
+          /* bFrameEnabled */false);
+          //END:RTC_ENABLE_BFRAME
 
   private static final int DECODE_TIMEOUT_MS = 1000;
   private static final VideoDecoder.Settings SETTINGS =
diff --git a/sdk/android/instrumentationtests/src/org/webrtc/HardwareVideoEncoderTest.java b/sdk/android/instrumentationtests/src/org/webrtc/HardwareVideoEncoderTest.java
index ab8490e..695fc3f 100644
--- a/sdk/android/instrumentationtests/src/org/webrtc/HardwareVideoEncoderTest.java
+++ b/sdk/android/instrumentationtests/src/org/webrtc/HardwareVideoEncoderTest.java
@@ -59,7 +59,7 @@ public class HardwareVideoEncoderTest {
   private static final VideoEncoder.Settings SETTINGS =
       new VideoEncoder.Settings(1 /* core */, 640 /* width */, 480 /* height */, 300 /* kbps */,
           30 /* fps */, 1 /* numberOfSimulcastStreams */, true /* automaticResizeOn */,
-          /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */));
+          /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */), /* bFramesEnabled */false);
   private static final int ENCODE_TIMEOUT_MS = 1000;
   private static final int NUM_TEST_FRAMES = 10;
   private static final int NUM_ENCODE_TRIES = 100;
diff --git a/sdk/android/src/java/org/webrtc/AndroidVideoDecoder.java b/sdk/android/src/java/org/webrtc/AndroidVideoDecoder.java
index 47cb568..dcb6d13 100644
--- a/sdk/android/src/java/org/webrtc/AndroidVideoDecoder.java
+++ b/sdk/android/src/java/org/webrtc/AndroidVideoDecoder.java
@@ -267,6 +267,7 @@ class AndroidVideoDecoder implements VideoDecoder, VideoSink {
 
     frameInfos.offer(new FrameInfo(SystemClock.elapsedRealtime(), frame.rotation));
     try {
+
       codec.queueInputBuffer(index, 0 /* offset */, size,
           TimeUnit.NANOSECONDS.toMicros(frame.captureTimeNs), 0 /* flags */);
     } catch (IllegalStateException e) {
@@ -409,6 +410,21 @@ class AndroidVideoDecoder implements VideoDecoder, VideoSink {
     }
 
     synchronized (renderedTextureMetadataLock) {
+      //START:RTC_ENABLE_BFRAME
+      // To prevent frames from being burst-generated when decoding B frames, 
+      // causing frame drops on the surface, wait until onFrame is called.
+      try {
+        long timeoutMillis = TimeUnit.MILLISECONDS.convert(DEQUEUE_OUTPUT_BUFFER_TIMEOUT_US, TimeUnit.MICROSECONDS);
+        long startTime = System.currentTimeMillis();
+        long elapsedTime = 0;
+        while (renderedTextureMetadata != null && elapsedTime < timeoutMillis) {
+          long waitTime = timeoutMillis - elapsedTime;
+          renderedTextureMetadataLock.wait(waitTime);
+          elapsedTime = System.currentTimeMillis() - startTime;
+        }
+      } catch(InterruptedException  e) { }
+      //END:RTC_ENABLE_BFRAME
+
       if (renderedTextureMetadata != null) {
         codec.releaseOutputBuffer(index, false);
         return; // We are still waiting for texture for the previous frame, drop this one.
@@ -433,6 +449,11 @@ class AndroidVideoDecoder implements VideoDecoder, VideoSink {
       timestampNs = renderedTextureMetadata.presentationTimestampUs * 1000;
       decodeTimeMs = renderedTextureMetadata.decodeTimeMs;
       renderedTextureMetadata = null;
+
+      //START:RTC_ENABLE_BFRAME
+      renderedTextureMetadataLock.notifyAll();
+      //END:RTC_ENABLE_BFRAME
+
     }
     // Change timestamp of frame.
     final VideoFrame frameWithModifiedTimeStamp =
diff --git a/sdk/android/src/java/org/webrtc/HardwareVideoEncoder.java b/sdk/android/src/java/org/webrtc/HardwareVideoEncoder.java
index 9f57b20..e116863 100644
--- a/sdk/android/src/java/org/webrtc/HardwareVideoEncoder.java
+++ b/sdk/android/src/java/org/webrtc/HardwareVideoEncoder.java
@@ -29,7 +29,9 @@ import java.util.Map;
 import java.util.concurrent.BlockingDeque;
 import java.util.concurrent.LinkedBlockingDeque;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.ConcurrentHashMap;
 import org.webrtc.ThreadUtils.ThreadChecker;
+import java.util.Vector;
 
 /**
  * Android hardware video encoder.
@@ -40,8 +42,10 @@ class HardwareVideoEncoder implements VideoEncoder {
   private static final int MAX_VIDEO_FRAMERATE = 30;
 
   // See MAX_ENCODER_Q_SIZE in androidmediaencoder.cc.
-  private static final int MAX_ENCODER_Q_SIZE = 2;
-
+  //START:RTC_ENABLE_BFRAME
+  //private static final int MAX_ENCODER_Q_SIZE = 2;
+  private static final int MAX_ENCODER_Q_SIZE = 4;
+  //STAENDT:RTC_ENABLE_BFRAME
   private static final int MEDIA_CODEC_RELEASE_TIMEOUT_MS = 5000;
   private static final int DEQUEUE_OUTPUT_BUFFER_TIMEOUT_US = 100000;
 
@@ -115,7 +119,15 @@ class HardwareVideoEncoder implements VideoEncoder {
   // A queue of EncodedImage.Builders that correspond to frames in the codec.  These builders are
   // pre-populated with all the information that can't be sent through MediaCodec.
   private final BlockingDeque<EncodedImage.Builder> outputBuilders = new LinkedBlockingDeque<>();
-
+  //START:RTC_ENABLE_BFRAME
+  // When BFrames are enabled, the order of decoded frames changes. 
+  // It is used for the purpose of finding the timestamp RTP value corresponding to the frame.
+  // <presentationTimestamp, timestampRtp>
+  private final ConcurrentHashMap<Long, Long> ptsMap = new ConcurrentHashMap<>();
+  private static final int MAX_TIMESTAMP_Q_SIZE = 16;
+  private boolean bframeEnabled;
+  private int     maxBframes;
+  //END:RTC_ENABLE_BFRAME
   private final ThreadChecker encodeThreadChecker = new ThreadChecker();
   private final ThreadChecker outputThreadChecker = new ThreadChecker();
   private final BusyCount outputBuffersBusyCount = new BusyCount();
@@ -203,6 +215,28 @@ class HardwareVideoEncoder implements VideoEncoder {
     encodeThreadChecker.detachThread();
   }
 
+
+  //START:RTC_ENABLE_BFRAME
+  public HardwareVideoEncoder(MediaCodecWrapperFactory mediaCodecWrapperFactory, String codecName,
+      VideoCodecMimeType codecType, Integer surfaceColorFormat, Integer yuvColorFormat,
+      Map<String, String> params, int keyFrameIntervalSec, int forceKeyFrameIntervalMs, int maxBframes, 
+      BitrateAdjuster bitrateAdjuster, EglBase14.Context sharedContext) {
+    this.mediaCodecWrapperFactory = mediaCodecWrapperFactory;
+    this.codecName = codecName;
+    this.codecType = codecType;
+    this.surfaceColorFormat = surfaceColorFormat;
+    this.yuvColorFormat = yuvColorFormat;
+    this.params = params;
+    this.keyFrameIntervalSec = keyFrameIntervalSec;
+    this.forcedKeyFrameNs = TimeUnit.MILLISECONDS.toNanos(forceKeyFrameIntervalMs);
+    this.bitrateAdjuster = bitrateAdjuster;
+    this.sharedContext = sharedContext;
+    this.maxBframes = maxBframes;
+    // Allow construction on a different thread.
+    encodeThreadChecker.detachThread();
+  }
+  //END:RTC_ENABLE_BFRAME
+
   @Override
   public VideoCodecStatus initEncode(Settings settings, Callback callback) {
     encodeThreadChecker.checkIsOnValidThread();
@@ -219,10 +253,18 @@ class HardwareVideoEncoder implements VideoEncoder {
     }
     adjustedBitrate = bitrateAdjuster.getAdjustedBitrateBps();
 
+    //START:RTC_ENABLE_BFRAME
+    this.bframeEnabled = settings.bframeEnabled;
+    //END:RTC_ENABLE_BFRAME
+
     Logging.d(TAG,
         "initEncode name: " + codecName + " type: " + codecType + " width: " + width
             + " height: " + height + " framerate_fps: " + settings.maxFramerate
-            + " bitrate_kbps: " + settings.startBitrate + " surface mode: " + useSurfaceMode);
+            + " bitrate_kbps: " + settings.startBitrate + " surface mode: " + useSurfaceMode
+    //START:RTC_ENABLE_BFRAME
+            + " bframe_enabled: " + settings.bframeEnabled + ", maxBframes: " + this.maxBframes);
+    //END:RTC_ENABLE_BFRAME
+            
     return initEncodeInternal();
   }
 
@@ -255,6 +297,29 @@ class HardwareVideoEncoder implements VideoEncoder {
         if (profileLevelId == null) {
           profileLevelId = VideoCodecInfo.H264_CONSTRAINED_BASELINE_3_1;
         }
+      
+        //START:RTC_ENABLE_BFRAME
+        if (this.bframeEnabled == true) {
+          String profile = profileLevelId.substring(0, 4);
+          String profileLevel = profileLevelId.substring(4, 6);
+
+          switch (profile) {
+            case VideoCodecInfo.H264_PROFILE_CONSTRAINED_HIGH:
+              format.setInteger("profile", AVCProfileHigh);
+              format.setInteger("level", AVCLevel3);
+              if (this.maxBframes > 0) {
+                format.setInteger(MediaFormat.KEY_MAX_B_FRAMES, this.maxBframes);
+                Logging.d(TAG, String.format("PROFILE_ID: %s, KEY_LEVEL:%d, KEY_MAX_B_FRAMES: %d",
+                  VideoCodecInfo.H264_CONSTRAINED_HIGH_3_1, AVCLevel3, this.maxBframes));              
+              }              
+              break;
+            case VideoCodecInfo.H264_PROFILE_CONSTRAINED_BASELINE:
+              break;
+            default:
+              Logging.w(TAG, "Unknown profile level id: " + profileLevelId);
+          }
+        } else {
+        //END:RTC_ENABLE_BFRAME
         switch (profileLevelId) {
           case VideoCodecInfo.H264_CONSTRAINED_HIGH_3_1:
             format.setInteger("profile", AVCProfileHigh);
@@ -265,7 +330,15 @@ class HardwareVideoEncoder implements VideoEncoder {
           default:
             Logging.w(TAG, "Unknown profile level id: " + profileLevelId);
         }
+      //START:RTC_ENABLE_BFRAME
+        }
+      } else if (codecType == VideoCodecMimeType.H265) {
+        if (this.bframeEnabled == true && this.maxBframes > 0) {
+            format.setInteger(MediaFormat.KEY_MAX_B_FRAMES, this.maxBframes);       
+        }
       }
+      //END:RTC_ENABLE_BFRAME
+
 
       if (codecName.equals("c2.google.av1.encoder")) {
         // Enable RTC mode in AV1 HW encoder.
@@ -339,7 +412,9 @@ class HardwareVideoEncoder implements VideoEncoder {
       textureInputSurface = null;
     }
     outputBuilders.clear();
-
+    //START:RTC_ENABLE_BFRAME
+    ptsMap.clear();
+    //END:RTC_ENABLE_BFRAME
     codec = null;
     outputThread = null;
 
@@ -391,6 +466,9 @@ class HardwareVideoEncoder implements VideoEncoder {
                                        .setEncodedWidth(videoFrame.getBuffer().getWidth())
                                        .setEncodedHeight(videoFrame.getBuffer().getHeight())
                                        .setRotation(videoFrame.getRotation());
+    //START:RTC_ENABLE_BFRAME
+    builder.setDecodingTimestamp(videoFrame.getTimestampRtp());
+    //END:RTC_ENABLE_BFRAME                                       
     outputBuilders.offer(builder);
 
     long presentationTimestampUs = nextPresentationTimestampUs;
@@ -399,6 +477,9 @@ class HardwareVideoEncoder implements VideoEncoder {
         (long) (TimeUnit.SECONDS.toMicros(1) / bitrateAdjuster.getAdjustedFramerateFps());
     nextPresentationTimestampUs += frameDurationUs;
 
+    //START:RTC_ENABLE_BFRAME
+    ptsMap.put(presentationTimestampUs, videoFrame.getTimestampRtp());
+    //END:RTC_ENABLE_BFRAME
     final VideoCodecStatus returnValue;
     if (useSurfaceMode) {
       returnValue = encodeTextureBuffer(videoFrame, presentationTimestampUs);
@@ -410,6 +491,10 @@ class HardwareVideoEncoder implements VideoEncoder {
     if (returnValue != VideoCodecStatus.OK) {
       // Keep the output builders in sync with buffers in the codec.
       outputBuilders.pollLast();
+
+      //START:RTC_ENABLE_BFRAME
+      ptsMap.remove(presentationTimestampUs);
+      //END:RTC_ENABLE_BFRAME
     }
 
     return returnValue;
@@ -658,10 +743,38 @@ class HardwareVideoEncoder implements VideoEncoder {
       final EncodedImage.FrameType frameType = isKeyFrame ? EncodedImage.FrameType.VideoFrameKey
                                                           : EncodedImage.FrameType.VideoFrameDelta;
 
+       //START:RTC_ENABLE_BFRAME
+      // When Bframe is enabled, encoded frames are created out of order.
+      // This is to find the original image timestamp(rtp) that matches the encoded image.
+      long timestampRtp = ptsMap.get(info.presentationTimeUs); 
+      ptsMap.remove(info.presentationTimeUs);
+
+      // Prevent timestamp map from continuously increasing.
+      while (ptsMap.size() >= MAX_TIMESTAMP_Q_SIZE) {
+        Long minKey = null;
+        Long minValue = Long.MAX_VALUE;
+
+        for (Map.Entry<Long, Long> entry : ptsMap.entrySet()) {
+            if (entry.getValue() < minValue) {
+                minKey = entry.getKey();
+                minValue = entry.getValue();
+            }
+        }
+
+        if (minKey != null) {
+          ptsMap.remove(minKey);
+        }
+      }
+      //END:RTC_ENABLE_BFRAME
+      
       EncodedImage.Builder builder = outputBuilders.poll();
       builder.setBuffer(frameBuffer, releaseCallback);
       builder.setFrameType(frameType);
       builder.setQp(qp);
+      //START:RTC_ENABLE_BFRAME
+      builder.setTimestampRtp(timestampRtp);
+      //END:RTC_ENABLE_BFRAME
+
 
       EncodedImage encodedImage = builder.createEncodedImage();
       // TODO(mellem):  Set codec-specific info.
diff --git a/sdk/android/src/java/org/webrtc/MediaCodecUtils.java b/sdk/android/src/java/org/webrtc/MediaCodecUtils.java
index 5417fec..2b96b9e 100644
--- a/sdk/android/src/java/org/webrtc/MediaCodecUtils.java
+++ b/sdk/android/src/java/org/webrtc/MediaCodecUtils.java
@@ -29,6 +29,9 @@ class MediaCodecUtils {
   static final String INTEL_PREFIX = "OMX.Intel.";
   static final String NVIDIA_PREFIX = "OMX.Nvidia.";
   static final String QCOM_PREFIX = "OMX.qcom.";
+  // Qualcomm codec name added in Android 10 and higher.
+  static final String QTI_PREFIX = "c2.qti.";
+  static final String C2_EXYNOS_PREFIX = "c2.exynos.";
   static final String[] SOFTWARE_IMPLEMENTATION_PREFIXES = {
       "OMX.google.", "OMX.SEC.", "c2.android"};
 
diff --git a/sdk/android/src/java/org/webrtc/MediaCodecVideoDecoderFactory.java b/sdk/android/src/java/org/webrtc/MediaCodecVideoDecoderFactory.java
index 9a73bc4..a31c19c 100644
--- a/sdk/android/src/java/org/webrtc/MediaCodecVideoDecoderFactory.java
+++ b/sdk/android/src/java/org/webrtc/MediaCodecVideoDecoderFactory.java
@@ -12,7 +12,8 @@ package org.webrtc;
 
 import static org.webrtc.MediaCodecUtils.EXYNOS_PREFIX;
 import static org.webrtc.MediaCodecUtils.QCOM_PREFIX;
-
+import static org.webrtc.MediaCodecUtils.QTI_PREFIX;
+import static org.webrtc.MediaCodecUtils.C2_EXYNOS_PREFIX;
 import android.media.MediaCodecInfo;
 import android.media.MediaCodecInfo.CodecCapabilities;
 import android.media.MediaCodecList;
@@ -79,7 +80,6 @@ class MediaCodecVideoDecoderFactory implements VideoDecoderFactory {
             name, MediaCodecUtils.getCodecProperties(type, /* highProfile= */ false)));
       }
     }
-
     return supportedCodecInfos.toArray(new VideoCodecInfo[supportedCodecInfos.size()]);
   }
 
@@ -128,7 +128,7 @@ class MediaCodecVideoDecoderFactory implements VideoDecoderFactory {
   private boolean isH264HighProfileSupported(MediaCodecInfo info) {
     String name = info.getName();
     // Support H.264 HP decoding on QCOM chips.
-    if (name.startsWith(QCOM_PREFIX)) {
+    if (name.startsWith(QCOM_PREFIX) || name.startsWith(QTI_PREFIX) || name.startsWith(C2_EXYNOS_PREFIX)) {
       return true;
     }
     // Support H.264 HP decoding on Exynos chips for Android M and above.
diff --git a/sdk/android/src/jni/encoded_image.cc b/sdk/android/src/jni/encoded_image.cc
index 9bd73a4..0add8d6 100644
--- a/sdk/android/src/jni/encoded_image.cc
+++ b/sdk/android/src/jni/encoded_image.cc
@@ -63,13 +63,19 @@ ScopedJavaLocalRef<jobject> NativeToJavaEncodedImage(
     qp = NativeToJavaInteger(jni, image.qp_);
   // TODO(bugs.webrtc.org/9378): Keep a reference to the C++ EncodedImage data,
   // and use the releaseCallback to manage lifetime.
-  return Java_EncodedImage_Constructor(
+  ScopedJavaLocalRef<jobject> encoded_image = Java_EncodedImage_Constructor(
       jni, buffer,
       /*releaseCallback=*/ScopedJavaGlobalRef<jobject>(nullptr),
       static_cast<int>(image._encodedWidth),
       static_cast<int>(image._encodedHeight),
       image.capture_time_ms_ * rtc::kNumNanosecsPerMillisec, frame_type,
       static_cast<jint>(image.rotation_), qp);
+
+    //START:RTC_ENABLE_BFRAME
+    Java_EncodedImage_setTimestampRtp(jni, encoded_image, static_cast<long>(image.RtpTimestamp()));
+    //END:RTC_ENABLE_BFRAME
+
+  return encoded_image;
 }
 
 ScopedJavaLocalRef<jobjectArray> NativeToJavaFrameTypeArray(
@@ -113,5 +119,18 @@ int64_t GetJavaEncodedImageCaptureTimeNs(
   return Java_EncodedImage_getCaptureTimeNs(env, j_encoded_image);
 }
 
+//START:RTC_ENABLE_BFRAME
+uint32_t GetJavaEncodedImageTimestampRtp(
+        JNIEnv* env,
+        const JavaRef<jobject>& j_encoded_image) {
+    return Java_EncodedImage_getTimestampRtp(env, j_encoded_image);
+}
+
+uint32_t GetJavaEncodedImageDecodingTimestamp(
+          JNIEnv* env,
+        const JavaRef<jobject>& j_encoded_image) {
+    return Java_EncodedImage_getDecodingTimestamp(env, j_encoded_image);
+//END:RTC_ENABLE_BFRAME    
+}
 }  // namespace jni
 }  // namespace webrtc
diff --git a/sdk/android/src/jni/encoded_image.h b/sdk/android/src/jni/encoded_image.h
index 2e89286..d6af4c7 100644
--- a/sdk/android/src/jni/encoded_image.h
+++ b/sdk/android/src/jni/encoded_image.h
@@ -39,6 +39,17 @@ int64_t GetJavaEncodedImageCaptureTimeNs(
     JNIEnv* jni,
     const JavaRef<jobject>& j_encoded_image);
 
+
+//START:RTC_ENABLE_BFRAME
+uint32_t GetJavaEncodedImageTimestampRtp(
+        JNIEnv* env,
+        const JavaRef<jobject>& j_encoded_image);
+
+uint32_t GetJavaEncodedImageDecodingTimestamp(
+          JNIEnv* env,
+        const JavaRef<jobject>& j_encoded_image);
+//END:RTC_ENABLE_BFRAME
+
 }  // namespace jni
 }  // namespace webrtc
 
diff --git a/sdk/android/src/jni/video_codec_info.cc b/sdk/android/src/jni/video_codec_info.cc
index 42e7b5d..49d8a8a 100644
--- a/sdk/android/src/jni/video_codec_info.cc
+++ b/sdk/android/src/jni/video_codec_info.cc
@@ -15,6 +15,7 @@
 #include "sdk/android/src/jni/jni_helpers.h"
 
 #include "absl/container/inlined_vector.h"
+#include "rtc_base/logging.h"
 
 namespace webrtc {
 namespace jni {
@@ -26,10 +27,22 @@ SdpVideoFormat VideoCodecInfoToSdpVideoFormat(JNIEnv* jni,
   for (const auto& scalabilityMode : javaScalabilityModes) {
     scalabilityModes.push_back(static_cast<webrtc::ScalabilityMode>(scalabilityMode));
   }
-  return SdpVideoFormat(
+  
+  SdpVideoFormat format = SdpVideoFormat(
       JavaToNativeString(jni, Java_VideoCodecInfo_getName(jni, j_info)),
       JavaToNativeStringMap(jni, Java_VideoCodecInfo_getParams(jni, j_info)),
       scalabilityModes);
+
+#ifdef RTC_ENABLE_BFRAME
+  [[maybe_unused]] bool bframe_enabled = (Java_VideoCodecInfo_getBframeEnabled(jni, j_info) != JNI_FALSE)?true:false;
+  format.bframe_enabled = bframe_enabled;
+  RTC_LOG(LS_VERBOSE) << "VideoCodecInfoToSdpVideoFormat. name:" << format.name.c_str() << ", bframe_enaled:" << (format.bframe_enabled?"True":"False");  
+#else
+  [[maybe_unused]] bool bframe_enabled = (Java_VideoCodecInfo_getBframeEnabled(jni, j_info) != JNI_FALSE)?true:false;
+  format.bframe_enabled = false;
+#endif
+
+  return format;
 }
 
 ScopedJavaLocalRef<jobject> SdpVideoFormatToVideoCodecInfo(
@@ -48,6 +61,13 @@ ScopedJavaLocalRef<jobject> SdpVideoFormatToVideoCodecInfo(
   }
   Java_VideoCodecInfo_setScalabilityModes(jni, codec, NativeToJavaIntArray(jni, temp));
 
+#ifdef RTC_ENABLE_BFRAME
+  RTC_LOG(LS_VERBOSE) << "SdpVideoFormatToVideoCodecInfo. name:" << format.name.c_str() << ", bframe_enaled:" << (format.bframe_enabled?"True":"False");
+  Java_VideoCodecInfo_setBframeEnabled(jni, codec, format.bframe_enabled?JNI_TRUE:JNI_FALSE);
+#else
+  Java_VideoCodecInfo_setBframeEnabled(jni, codec, JNI_FALSE);
+#endif
+
   return codec;
 }
 
diff --git a/sdk/android/src/jni/video_decoder_factory_wrapper.cc b/sdk/android/src/jni/video_decoder_factory_wrapper.cc
index 2d92404..e0cf282 100644
--- a/sdk/android/src/jni/video_decoder_factory_wrapper.cc
+++ b/sdk/android/src/jni/video_decoder_factory_wrapper.cc
@@ -42,9 +42,33 @@ std::unique_ptr<VideoDecoder> VideoDecoderFactoryWrapper::CreateVideoDecoder(
 std::vector<SdpVideoFormat> VideoDecoderFactoryWrapper::GetSupportedFormats()
     const {
   JNIEnv* env = AttachCurrentThreadIfNeeded();
+#ifdef RTC_ENABLE_BFRAME
+  std::vector<SdpVideoFormat> supported_formats = JavaToNativeVector<SdpVideoFormat>(
+      env, Java_VideoDecoderFactory_getSupportedCodecs(env, decoder_factory_),
+      &VideoCodecInfoToSdpVideoFormat);
+
+  for (auto &format : supported_formats) {
+      if (format.name == "H264") {
+        const auto profile_level_id_iter = format.parameters.find("profile-level-id");
+        if (profile_level_id_iter != format.parameters.end()) {
+          // refer to the H264Utils.java
+          const auto profile_level_id = profile_level_id_iter->second;
+          if (profile_level_id == "640c1f") {
+            format.bframe_enabled = true;
+          }
+        }
+      } else if (format.name == "H265") {
+        format.bframe_enabled = true;
+      } else {
+        format.bframe_enabled = false;
+      }
+  }
+  return supported_formats;
+#else
   return JavaToNativeVector<SdpVideoFormat>(
       env, Java_VideoDecoderFactory_getSupportedCodecs(env, decoder_factory_),
       &VideoCodecInfoToSdpVideoFormat);
+#endif      
 }
 
 }  // namespace jni
diff --git a/sdk/android/src/jni/video_decoder_wrapper.cc b/sdk/android/src/jni/video_decoder_wrapper.cc
index e083df5..a38b217 100644
--- a/sdk/android/src/jni/video_decoder_wrapper.cc
+++ b/sdk/android/src/jni/video_decoder_wrapper.cc
@@ -109,10 +109,32 @@ int32_t VideoDecoderWrapper::Decode(const EncodedImage& image_param,
   frame_extra_info.timestamp_ntp = input_image.ntp_time_ms_;
   frame_extra_info.qp =
       qp_parsing_enabled_ ? ParseQP(input_image) : absl::nullopt;
+
+#ifdef RTC_ENABLE_BFRAME
   {
+    // It already has been set by the native
+    frame_extra_info.decoding_timestamp = image_param.Dts().value_or(image_param.RtpTimestamp());
+    frame_extra_info.is_bframe = input_image.IsBFrame().value_or(false);
+
     MutexLock lock(&frame_extra_infos_lock_);
-    frame_extra_infos_.push_back(frame_extra_info);
+    frame_extra_infos_.insert(std::make_pair(frame_extra_info.timestamp_ns, frame_extra_info));
+    // In order for timestamp_ns to be created sequentially when calling OnDecodedFrame, 
+    // Presestation Timestamp must be calculated using CompositionTimestamp.
+    // PTS(capture_time_ms_) = CTS(CompositionTimestamp) * 90 + DTS(RtpTimestamp)
+    RTC_LOG(LS_VERBOSE)  << "Decode." 
+                    << "  timestamp_ns:" << frame_extra_info.timestamp_ns
+                    << " capture:" << input_image.capture_time_ms_ 
+                    << ", pts:" << image_param.RtpTimestamp() 
+                    << ", dts:" << image_param.Dts().value_or(0)
+                    << ", key:" << image_param.FrameType()
+                    << ", bframe: " << image_param.IsBFrame().value_or(false);
   }
+#else
+  {
+    MutexLock lock(&frame_extra_infos_lock_);
+    frame_extra_infos_.push_back(frame_extra_info);
+  } 
+#endif
 
   JNIEnv* env = AttachCurrentThreadIfNeeded();
   ScopedJavaLocalRef<jobject> jinput_image =
@@ -165,6 +187,34 @@ void VideoDecoderWrapper::OnDecodedFrame(
   const int64_t timestamp_ns = GetJavaVideoFrameTimestampNs(env, j_frame);
 
   FrameExtraInfo frame_extra_info;
+#ifdef RTC_ENABLE_BFRAME  
+  {
+    MutexLock lock(&frame_extra_infos_lock_);
+
+    auto it = frame_extra_infos_.find(timestamp_ns);
+    if (it == frame_extra_infos_.end()) {
+      RTC_LOG(LS_WARNING)
+          << "Java decoder produced an unexpected frame: " << timestamp_ns;
+      return;
+    }
+    frame_extra_info = it->second;
+    frame_extra_infos_.erase(it);
+    uint32_t pts = frame_extra_info.timestamp_rtp;
+    RTC_LOG(LS_VERBOSE)  << "Decoded." 
+                      << " timestamp_ns:" << timestamp_ns
+                      << ", pts:" << pts
+                      << ", decoded_time_ms: " << JavaToNativeOptionalInt(env, j_decode_time_ms).value_or(-1)
+                      << ". queue: " << frame_extra_infos_.size();
+
+    const auto count = std::erase_if(frame_extra_infos_, [pts](const auto& item) {
+        auto const& [timestamp_ns, frame_extra_info] = item;
+        return frame_extra_info.timestamp_rtp < pts;
+    });
+    if (count != 0) {
+        RTC_LOG(LS_WARNING) << "Removes " << count << "  dropped FrameExtraInfo.";
+    }
+  }
+#else
   {
     MutexLock lock(&frame_extra_infos_lock_);
 
@@ -181,13 +231,18 @@ void VideoDecoderWrapper::OnDecodedFrame(
       // find a matching timestamp.
     } while (frame_extra_info.timestamp_ns != timestamp_ns);
   }
+#endif
 
   VideoFrame frame =
       JavaToNativeFrame(env, j_frame, frame_extra_info.timestamp_rtp);
   frame.set_ntp_time_ms(frame_extra_info.timestamp_ntp);
 
+#ifdef RTC_ENABLE_BFRAME  
+  absl::optional<int32_t> decoding_time_ms = -1;
+#else
   absl::optional<int32_t> decoding_time_ms =
       JavaToNativeOptionalInt(env, j_decode_time_ms);
+#endif
 
   absl::optional<uint8_t> decoder_qp =
       cast_optional<uint8_t, int32_t>(JavaToNativeOptionalInt(env, j_qp));
diff --git a/sdk/android/src/jni/video_decoder_wrapper.h b/sdk/android/src/jni/video_decoder_wrapper.h
index 53246f3..e80a106 100644
--- a/sdk/android/src/jni/video_decoder_wrapper.h
+++ b/sdk/android/src/jni/video_decoder_wrapper.h
@@ -67,6 +67,11 @@ class VideoDecoderWrapper : public VideoDecoder {
     int64_t timestamp_ntp;
     absl::optional<uint8_t> qp;
 
+#ifdef RTC_ENABLE_BFRAME
+    uint32_t decoding_timestamp;
+    bool is_bframe = false;
+#endif
+
     FrameExtraInfo();
     FrameExtraInfo(const FrameExtraInfo&);
     ~FrameExtraInfo();
@@ -109,7 +114,11 @@ class VideoDecoderWrapper : public VideoDecoder {
   // Accessed both on the decoder thread and the callback thread.
   std::atomic<bool> qp_parsing_enabled_;
   Mutex frame_extra_infos_lock_;
+#ifdef RTC_ENABLE_BFRAME
+  std::map<int64_t, FrameExtraInfo> frame_extra_infos_
+#else
   std::deque<FrameExtraInfo> frame_extra_infos_
+#endif      
       RTC_GUARDED_BY(frame_extra_infos_lock_);
 };
 
diff --git a/sdk/android/src/jni/video_encoder_wrapper.cc b/sdk/android/src/jni/video_encoder_wrapper.cc
index ace53c9..92ad7d2 100644
--- a/sdk/android/src/jni/video_encoder_wrapper.cc
+++ b/sdk/android/src/jni/video_encoder_wrapper.cc
@@ -13,6 +13,9 @@
 #include <utility>
 
 #include "common_video/h264/h264_common.h"
+#ifdef RTC_ENABLE_BFRAME
+#include "common_video/h265/h265_common.h"
+#endif
 #include "modules/video_coding/include/video_codec_interface.h"
 #include "modules/video_coding/include/video_error_codes.h"
 #include "modules/video_coding/svc/scalable_video_controller_no_layering.h"
@@ -69,6 +72,14 @@ int32_t VideoEncoderWrapper::InitEncodeInternal(JNIEnv* jni) {
       automatic_resize_on = true;
   }
 
+//START:RTC_ENABLE_BFRAME
+  bool bframe_enabled = false;
+//END:RTC_ENABLE_BFRAME  
+#ifdef RTC_ENABLE_BFRAME
+  bframe_enabled = IsBFrameEnabled();
+  RTC_LOG(LS_INFO) << "VideoEncoderWrapper::InitEncodeInternal bframe_enabled:" << (bframe_enabled?"True":"False"); 
+#endif
+  
   RTC_DCHECK(capabilities_);
   ScopedJavaLocalRef<jobject> capabilities =
       Java_Capabilities_Constructor(jni, capabilities_->loss_notification);
@@ -78,7 +89,11 @@ int32_t VideoEncoderWrapper::InitEncodeInternal(JNIEnv* jni) {
       static_cast<int>(codec_settings_.startBitrate),
       static_cast<int>(codec_settings_.maxFramerate),
       static_cast<int>(codec_settings_.numberOfSimulcastStreams),
-      automatic_resize_on, capabilities);
+      automatic_resize_on, capabilities, 
+//START:RTC_ENABLE_BFRAME      
+      bframe_enabled
+//END:RTC_ENABLE_BFRAME
+      );
 
   ScopedJavaLocalRef<jobject> callback =
       Java_VideoEncoderWrapper_createEncoderCallback(jni,
@@ -131,10 +146,12 @@ int32_t VideoEncoderWrapper::Release() {
   int32_t status = JavaToNativeVideoCodecStatus(
       jni, Java_VideoEncoder_release(jni, encoder_));
   RTC_LOG(LS_INFO) << "release: " << status;
+#ifndef RTC_ENABLE_BFRAME  
   {
     MutexLock lock(&frame_extra_infos_lock_);
     frame_extra_infos_.clear();
   }
+#endif  
   initialized_ = false;
 
   return status;
@@ -161,6 +178,7 @@ int32_t VideoEncoderWrapper::Encode(
   ScopedJavaLocalRef<jobject> encode_info =
       Java_EncodeInfo_Constructor(jni, j_frame_types);
 
+#ifndef RTC_ENABLE_BFRAME
   FrameExtraInfo info;
   info.capture_time_ns = frame.timestamp_us() * rtc::kNumNanosecsPerMicrosec;
   info.timestamp_rtp = frame.timestamp();
@@ -168,6 +186,7 @@ int32_t VideoEncoderWrapper::Encode(
     MutexLock lock(&frame_extra_infos_lock_);
     frame_extra_infos_.push_back(info);
   }
+#endif  
 
   ScopedJavaLocalRef<jobject> j_frame = NativeToJavaVideoFrame(jni, frame);
   ScopedJavaLocalRef<jobject> ret =
@@ -267,7 +286,15 @@ void VideoEncoderWrapper::OnEncodedFrame(
   EncodedImage frame = JavaToNativeEncodedImage(jni, j_encoded_image);
   int64_t capture_time_ns =
       GetJavaEncodedImageCaptureTimeNs(jni, j_encoded_image);
+  //START:RTC_ENABLE_BFRAME      
+  [[maybe_unused]] uint32_t timestamp_rtp = 
+      GetJavaEncodedImageTimestampRtp(jni, j_encoded_image);
 
+  [[maybe_unused]] uint32_t decoding_timestamp = 
+      GetJavaEncodedImageDecodingTimestamp(jni, j_encoded_image);
+  //END:RTC_ENABLE_BFRAME      
+
+#ifndef RTC_ENABLE_BFRAME
   // Encoded frames are delivered in the order received, but some of them
   // may be dropped, so remove records of frames older than the current
   // one.
@@ -296,6 +323,7 @@ void VideoEncoderWrapper::OnEncodedFrame(
     frame_extra_info = frame_extra_infos_.front();
     frame_extra_infos_.pop_front();
   }
+#endif
 
   // This is a bit subtle. The `frame` variable from the lambda capture is
   // const. Which implies that (i) we need to make a copy to be able to
@@ -305,12 +333,31 @@ void VideoEncoderWrapper::OnEncodedFrame(
   // CopyOnWriteBuffer.
   EncodedImage frame_copy = frame;
 
-  frame_copy.SetRtpTimestamp(frame_extra_info.timestamp_rtp);
   frame_copy.capture_time_ms_ = capture_time_ns / rtc::kNumNanosecsPerMillisec;
 
   if (frame_copy.qp_ < 0)
     frame_copy.qp_ = ParseQp(frame);
 
+#ifdef RTC_ENABLE_BFRAME
+  frame_copy.SetRtpTimestamp(timestamp_rtp);
+  if (IsBFrameEnabled()) {
+    frame_copy.SetDts(decoding_timestamp);
+    frame_copy.SetBFrame(IsBFrame());      
+  } else {
+    frame_copy.SetBFrame(false);
+  }
+
+  RTC_LOG(LS_VERBOSE) << "Encoded." 
+                    << " capture: " << frame_copy.capture_time_ms_
+                    << " pts:" << timestamp_rtp 
+                    << " dts:" << decoding_timestamp
+                    << "(" << ((double)timestamp_rtp - (double)decoding_timestamp) << ")"
+                    << " slice:" << GetSliceType().value_or(-1)
+                    << " bframe:" << IsBFrame();
+#else
+  frame_copy.SetRtpTimestamp(frame_extra_info.timestamp_rtp);
+#endif
+
   CodecSpecificInfo info(ParseCodecSpecificInfo(frame));
 
   if (callback_ != nullptr) {
@@ -373,6 +420,45 @@ int VideoEncoderWrapper::ParseQp(rtc::ArrayView<const uint8_t> buffer) {
   return success ? qp : -1;  // -1 means unknown QP.
 }
 
+#ifdef RTC_ENABLE_BFRAME
+// It must be called after ParseQp.
+absl::optional<uint8_t> VideoEncoderWrapper::GetSliceType() const {
+  switch (codec_settings_.codecType) {
+    case kVideoCodecH264:
+      return h264_bitstream_parser_.GetLastSliceType();
+    case kVideoCodecH265:
+      return h265_bitstream_parser_.GetLastSliceType();
+    default:
+      return 0xFF;
+  }
+}
+
+bool VideoEncoderWrapper::IsBFrame() const {
+  absl::optional<uint8_t> slice_type;
+  switch (codec_settings_.codecType) {
+    case kVideoCodecH264:
+      slice_type = GetSliceType();
+      return (slice_type.has_value() && slice_type.value() == H264::SliceType::kB);
+    case kVideoCodecH265:
+      slice_type = GetSliceType();
+      return (slice_type.has_value() && slice_type.value() == H265::SliceType::kB);
+    default:
+      return false;
+  }
+}
+
+bool VideoEncoderWrapper::IsBFrameEnabled() const {
+ switch (codec_settings_.codecType) {
+    case kVideoCodecH264:
+      return codec_settings_.H264().bframe_enabled;
+    case kVideoCodecH265:
+      return codec_settings_.H265().bframe_enabled;
+    default:
+      return false;
+  }
+}
+#endif
+
 CodecSpecificInfo VideoEncoderWrapper::ParseCodecSpecificInfo(
     const EncodedImage& frame) {
   const bool key_frame = frame._frameType == VideoFrameType::kVideoFrameKey;
diff --git a/sdk/android/src/jni/video_encoder_wrapper.h b/sdk/android/src/jni/video_encoder_wrapper.h
index 04d70f3..3d0df8f 100644
--- a/sdk/android/src/jni/video_encoder_wrapper.h
+++ b/sdk/android/src/jni/video_encoder_wrapper.h
@@ -73,6 +73,13 @@ class VideoEncoderWrapper : public VideoEncoder {
 
   int ParseQp(rtc::ArrayView<const uint8_t> buffer);
 
+#ifdef RTC_ENABLE_BFRAME
+  bool IsBFrameEnabled() const;
+  // // It must be called after ParseQp.
+  absl::optional<uint8_t> GetSliceType() const;
+  bool IsBFrame() const;
+#endif
+
   CodecSpecificInfo ParseCodecSpecificInfo(const EncodedImage& frame);
 
   ScopedJavaLocalRef<jobject> ToJavaBitrateAllocation(
@@ -94,10 +101,12 @@ class VideoEncoderWrapper : public VideoEncoder {
   const ScopedJavaGlobalRef<jobject> encoder_;
   const ScopedJavaGlobalRef<jclass> int_array_class_;
 
+#ifndef RTC_ENABLE_BFRAME
   // Modified both on the encoder thread and the callback thread.
   Mutex frame_extra_infos_lock_;
   std::deque<FrameExtraInfo> frame_extra_infos_
       RTC_GUARDED_BY(frame_extra_infos_lock_);
+#endif
   EncodedImageCallback* callback_;
   bool initialized_;
   int num_resets_;
diff --git a/sdk/android/src/jni/video_frame.cc b/sdk/android/src/jni/video_frame.cc
index 121b34f..f3ec03a 100644
--- a/sdk/android/src/jni/video_frame.cc
+++ b/sdk/android/src/jni/video_frame.cc
@@ -180,6 +180,15 @@ int64_t GetJavaVideoFrameTimestampNs(JNIEnv* jni,
   return Java_VideoFrame_getTimestampNs(jni, j_video_frame);
 }
 
+//START:RTC_ENABLE_BFRAME
+uint32_t GetJavaVideoFrameTimestampRtp(JNIEnv* jni,
+                                     const JavaRef<jobject>& j_video_frame) {
+
+  return Java_VideoFrame_getTimestampRtp(jni, j_video_frame);
+}
+//END:RTC_ENABLE_BFRAME
+
+
 rtc::scoped_refptr<AndroidVideoBuffer> AndroidVideoBuffer::Adopt(
     JNIEnv* jni,
     const JavaRef<jobject>& j_video_frame_buffer) {
@@ -298,16 +307,24 @@ ScopedJavaLocalRef<jobject> NativeToJavaVideoFrame(JNIEnv* jni,
     ScopedJavaLocalRef<jobject> j_video_frame_buffer(
         jni, android_buffer->video_frame_buffer());
     Java_Buffer_retain(jni, j_video_frame_buffer);
-    return Java_VideoFrame_Constructor(
+    ScopedJavaLocalRef<jobject> j_video_frame = Java_VideoFrame_Constructor(
         jni, j_video_frame_buffer, static_cast<jint>(frame.rotation()),
         static_cast<jlong>(frame.timestamp_us() *
                            rtc::kNumNanosecsPerMicrosec));
+    //START:RTC_ENABLE_BFRAME
+    Java_VideoFrame_setTimestampRtp(jni, j_video_frame, static_cast<jlong>(frame.timestamp()));
+    //END:RTC_ENABLE_BFRAME
+    return j_video_frame;
   } else {
-    return Java_VideoFrame_Constructor(
+    ScopedJavaLocalRef<jobject> j_video_frame =  Java_VideoFrame_Constructor(
         jni, WrapI420Buffer(jni, buffer->ToI420()),
         static_cast<jint>(frame.rotation()),
         static_cast<jlong>(frame.timestamp_us() *
                            rtc::kNumNanosecsPerMicrosec));
+    //START:RTC_ENABLE_BFRAME
+    Java_VideoFrame_setTimestampRtp(jni, j_video_frame, static_cast<jlong>(frame.timestamp()));
+    //END:RTC_ENABLE_BFRAME
+    return j_video_frame;                           
   }
 }
 
diff --git a/sdk/android/src/jni/video_frame.h b/sdk/android/src/jni/video_frame.h
index 9b916de..1281a5b 100644
--- a/sdk/android/src/jni/video_frame.h
+++ b/sdk/android/src/jni/video_frame.h
@@ -37,6 +37,12 @@ void ReleaseJavaVideoFrame(JNIEnv* jni, const JavaRef<jobject>& j_video_frame);
 int64_t GetJavaVideoFrameTimestampNs(JNIEnv* jni,
                                      const JavaRef<jobject>& j_video_frame);
 
+//START:RTC_ENABLE_BFRAME
+uint32_t GetJavaVideoFrameTimestampRtp(JNIEnv* jni,
+                                     const JavaRef<jobject>& j_video_frame);
+//END:RTC_ENABLE_BFRAME
+
+
 }  // namespace jni
 }  // namespace webrtc
 
diff --git a/sdk/android/tests/src/org/webrtc/HardwareVideoEncoderTest.java b/sdk/android/tests/src/org/webrtc/HardwareVideoEncoderTest.java
index 3c24e00..f180c24 100644
--- a/sdk/android/tests/src/org/webrtc/HardwareVideoEncoderTest.java
+++ b/sdk/android/tests/src/org/webrtc/HardwareVideoEncoderTest.java
@@ -75,7 +75,11 @@ public class HardwareVideoEncoderTest {
       /* maxFramerate= */ 30,
       /* numberOfSimulcastStreams= */ 1,
       /* automaticResizeOn= */ true,
-      /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */));
+      /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */), 
+      //START:RTC_ENABLE_BFRAME
+      /* bFrameEnabled */false);
+      //END:RTC_ENABLE_BFRAME
+
   private static final long POLL_DELAY_MS = 10;
   private static final long DELIVER_ENCODED_IMAGE_DELAY_MS = 10;
   private static final EncodeInfo ENCODE_INFO_KEY_FRAME =
@@ -90,11 +94,18 @@ public class HardwareVideoEncoderTest {
 
     TestEncoder(MediaCodecWrapperFactory mediaCodecWrapperFactory, String codecName,
         VideoCodecMimeType codecType, Integer surfaceColorFormat, Integer yuvColorFormat,
-        Map<String, String> params, int keyFrameIntervalSec, int forceKeyFrameIntervalMs,
+        Map<String, String> params, int keyFrameIntervalSec, int forceKeyFrameIntervalMs, 
+        //START:RTC_ENABLE_BFRAME
+        int maxBFrames,
+        //END:RTC_ENABLE_BFRAME
         BitrateAdjuster bitrateAdjuster, EglBase14.Context sharedContext,
         boolean isEncodingStatisticsSupported) {
       super(mediaCodecWrapperFactory, codecName, codecType, surfaceColorFormat, yuvColorFormat,
-          params, keyFrameIntervalSec, forceKeyFrameIntervalMs, bitrateAdjuster, sharedContext);
+          params, keyFrameIntervalSec, forceKeyFrameIntervalMs, 
+          //START:RTC_ENABLE_BFRAME
+          maxBFrames, 
+          //END:RTC_ENABLE_BFRAME
+          bitrateAdjuster, sharedContext);
       this.isEncodingStatisticsSupported = isEncodingStatisticsSupported;
     }
 
@@ -179,7 +190,11 @@ public class HardwareVideoEncoderTest {
           /* surfaceColorFormat= */ null, colorFormat,
           /* params= */ new HashMap<>(),
           /* keyFrameIntervalSec= */ 0,
-          /* forceKeyFrameIntervalMs= */ 0, bitrateAdjuster,
+          /* forceKeyFrameIntervalMs= */ 0,
+          //START:RTC_ENABLE_BFRAME
+          /* maxBFrames */ 0,
+          //EMD:RTC_ENABLE_BFRAME
+          bitrateAdjuster,
           /* sharedContext= */ null, isEncodingStatisticsSupported);
     }
   }
@@ -421,7 +436,10 @@ public class HardwareVideoEncoderTest {
             /* maxFramerate= */ 15,
             /* numberOfSimulcastStreams= */ 1,
             /* automaticResizeOn= */ true,
-            /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */)),
+            /* capabilities= */ new VideoEncoder.Capabilities(false /* lossNotification */),
+            //START:RTC_ENABLE_BFRAME
+            /* maxBFrames */ false),
+            //END:RTC_ENABLE_BFRAME
         mockEncoderCallback);
 
     MediaFormat mediaFormat = fakeMediaCodecWrapper.getConfiguredFormat();
